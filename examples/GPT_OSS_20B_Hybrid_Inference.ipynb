{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# GPT-OSS 20B Hybrid Inference: JAX Loading + MaxText Pallas INT8 Kernels\n",
    "\n",
    "This notebook combines the best approaches from two strategies:\n",
    "1. **JAX/Flax loading pipeline** (CPU â†’ Orbax â†’ TPU, no GPU needed)\n",
    "2. **MaxText Pallas kernels** (true INT8 compute on TPU v6e)\n",
    "\n",
    "## Key Innovations\n",
    "\n",
    "### ðŸš€ CPU â†’ TPU Loading (No GPU Required)\n",
    "- Load 42GB BF16 model on CPU\n",
    "- Quantize to 21GB INT8 during load\n",
    "- Transfer to TPU using Orbax structure-aware restore\n",
    "- **Fits in 32GB TPU v6e memory!**\n",
    "\n",
    "### âš¡ True INT8 Compute with Pallas\n",
    "- Use MaxText's Pallas kernels for native INT8 matmul\n",
    "- **2x throughput** vs BF16 on TPU v6e (150-180 tok/s)\n",
    "- No upcasting to BF16 (unlike JAX's FP8)\n",
    "- <1% quality loss with per-channel quantization\n",
    "\n",
    "### ðŸ’¾ Memory Optimization\n",
    "- INT8 weights: 21GB (50% reduction)\n",
    "- INT8 KV cache: 4x memory savings for long contexts\n",
    "- Supports up to 128K tokens on v6e-8\n",
    "\n",
    "## Requirements\n",
    "\n",
    "| TPU Type | Memory | Precision | Expected Performance |\n",
    "|----------|--------|-----------|---------------------|\n",
    "| **v6e-8** | 32GB | INT8 + Pallas | 150-180 tok/s âš¡ |\n",
    "| v2-8, v3-8 | 64GB | BF16 | 80-100 tok/s |\n",
    "\n",
    "**Dependencies**: HuggingFace account, ~50GB disk space\n",
    "\n",
    "## Workflow\n",
    "\n",
    "```\n",
    "HuggingFace (MXFP4)\n",
    "       â†“\n",
    "  CPU Loading â†’ MXFP4 decompression â†’ INT8 quantization\n",
    "       â†“\n",
    "  Orbax checkpoint (with structure metadata)\n",
    "       â†“\n",
    "  TPU Loading â†’ Pallas INT8 kernels â†’ Harmony inference âœ…\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Generated with Claude Code  \n",
    "**License**: MIT  \n",
    "**Last Updated**: 2025-11-17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-1-header"
   },
   "source": [
    "## Cell 1: TPU Detection and Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-1"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Detect TPU devices\n",
    "devices = jax.devices()\n",
    "print(f\"Available devices: {devices}\")\n",
    "print(f\"Device count: {len(devices)}\")\n",
    "\n",
    "if not devices or devices[0].platform != 'tpu':\n",
    "    print(\"\\nâš ï¸  WARNING: Not running on TPU!\")\n",
    "    print(\"   Please change runtime to TPU:\")\n",
    "    print(\"   Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ TPU\")\n",
    "    raise RuntimeError(\"TPU not detected\")\n",
    "\n",
    "# Get TPU type and memory capacity\n",
    "tpu_type = devices[0].device_kind\n",
    "print(f\"\\nâœ… Running on {tpu_type}\")\n",
    "\n",
    "# Estimate memory capacity based on TPU type\n",
    "if 'v6' in tpu_type.lower():\n",
    "    tpu_memory_gb = 32  # v6e-8 has 32GB HBM\n",
    "    recommended_dtype = 'int8'\n",
    "    recommended_strategy = 'INT8 + Pallas kernels (21GB model)'\n",
    "elif 'v5' in tpu_type.lower():\n",
    "    tpu_memory_gb = 16 if 'lite' in tpu_type.lower() else 95\n",
    "    recommended_dtype = 'int8' if tpu_memory_gb < 40 else 'bfloat16'\n",
    "    recommended_strategy = 'INT8 + Pallas' if tpu_memory_gb < 40 else 'BF16'\n",
    "elif 'v2' in tpu_type.lower() or 'v3' in tpu_type.lower() or 'v4' in tpu_type.lower():\n",
    "    tpu_memory_gb = 64  # v2-8, v3-8, v4-8 have 64GB HBM\n",
    "    recommended_dtype = 'bfloat16'\n",
    "    recommended_strategy = 'BF16 full precision (42GB model)'\n",
    "else:\n",
    "    tpu_memory_gb = 32  # Conservative estimate\n",
    "    recommended_dtype = 'int8'\n",
    "    recommended_strategy = 'INT8 + Pallas (safe for most TPUs)'\n",
    "\n",
    "print(f\"\\nTPU Memory: ~{tpu_memory_gb}GB HBM\")\n",
    "print(f\"Recommended precision: {recommended_dtype}\")\n",
    "print(f\"Recommended strategy: {recommended_strategy}\")\n",
    "\n",
    "# Memory requirements\n",
    "print(\"\\nðŸ“Š Model Memory Requirements:\")\n",
    "print(\"   BF16:  ~42GB (full precision)\")\n",
    "print(\"   INT8:  ~21GB (quantized with Pallas)\")\n",
    "print(\"   FP8:   ~21GB (but upcast to BF16 in JAX = OOM!)\")\n",
    "\n",
    "if tpu_memory_gb < 30:\n",
    "    print(\"\\nâš ï¸  WARNING: TPU has <30GB memory\")\n",
    "    print(\"   This may not be sufficient for GPT-OSS 20B\")\n",
    "    print(\"   Recommended: TPU v6e-8 (32GB) or v2-8 (64GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-2-header"
   },
   "source": [
    "## Cell 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-2"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Upgrade pip\n",
    "pip install -U pip\n",
    "\n",
    "# JAX ecosystem for TPU\n",
    "pip install -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "pip install -U flax optax\n",
    "\n",
    "# Checkpoint and serialization\n",
    "pip install -U orbax-checkpoint safetensors\n",
    "\n",
    "# Model-specific dependencies\n",
    "pip install -U transformers tiktoken\n",
    "\n",
    "# HuggingFace utilities\n",
    "pip install -U huggingface-hub\n",
    "\n",
    "# Clone gpt-oss-jax repo (for model architecture and loading)\n",
    "if [ ! -d \"/content/gpt-oss-jax\" ]; then\n",
    "  git clone https://github.com/atsentia/gpt-oss-jax.git /content/gpt-oss-jax\n",
    "fi\n",
    "cd /content/gpt-oss-jax && pip install -e \".[jax]\"\n",
    "\n",
    "# Clone MaxText repo (for Pallas INT8 kernels)\n",
    "if [ ! -d \"/content/maxtext\" ]; then\n",
    "  git clone https://github.com/AI-Hypercomputer/maxtext.git /content/maxtext\n",
    "fi\n",
    "cd /content/maxtext && pip install -r requirements.txt\n",
    "\n",
    "echo \"âœ… All dependencies installed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-3-header"
   },
   "source": [
    "## Cell 3: User Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ============ USER CONFIGURATION ============\n",
    "\n",
    "# HuggingFace token (required for model download)\n",
    "# Get from: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"\"  # â† Fill this in!\n",
    "\n",
    "# Precision mode\n",
    "# - \"auto\": Auto-select based on TPU type (recommended)\n",
    "# - \"bfloat16\": Full precision (requires 64GB TPU)\n",
    "# - \"int8\": Quantized with Pallas kernels (fits in 32GB TPU)\n",
    "PRECISION_MODE = \"auto\"\n",
    "\n",
    "# Advanced settings\n",
    "ENABLE_PALLAS_KERNELS = True  # Use MaxText INT8 kernels (recommended for v6e)\n",
    "QUANTIZE_KV_CACHE = True  # INT8 KV cache for long contexts\n",
    "KV_CACHE_DTYPE = \"int8\"  # \"int8\" or \"int4\" (int4 = 2x memory savings)\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Validation\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\n",
    "        \"Please set HF_TOKEN above.\\n\"\n",
    "        \"Get your token from: https://huggingface.co/settings/tokens\"\n",
    "    )\n",
    "\n",
    "# Set environment variable for HuggingFace\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "os.environ['HUGGING_FACE_HUB_TOKEN'] = HF_TOKEN\n",
    "\n",
    "print(\"âœ… Configuration validated\")\n",
    "print(f\"   Precision mode: {PRECISION_MODE}\")\n",
    "print(f\"   Pallas kernels: {ENABLE_PALLAS_KERNELS}\")\n",
    "print(f\"   KV cache quantization: {QUANTIZE_KV_CACHE} ({KV_CACHE_DTYPE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-4-header"
   },
   "source": [
    "## Cell 4: Precision Selection Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-4"
   },
   "outputs": [],
   "source": [
    "# Determine target dtype based on configuration and TPU type\n",
    "if PRECISION_MODE == \"auto\":\n",
    "    if tpu_memory_gb >= 60:\n",
    "        # v2-8, v3-8, v4-8: Use BF16 (full precision)\n",
    "        target_dtype = jnp.bfloat16\n",
    "        use_pallas = False\n",
    "        strategy_name = \"BF16 (full precision)\"\n",
    "        expected_memory_gb = 42\n",
    "        expected_throughput = \"80-100 tokens/sec\"\n",
    "    elif tpu_memory_gb >= 30:\n",
    "        # v6e-8: Use INT8 with Pallas\n",
    "        target_dtype = jnp.int8\n",
    "        use_pallas = ENABLE_PALLAS_KERNELS\n",
    "        strategy_name = \"INT8 + Pallas kernels\"\n",
    "        expected_memory_gb = 21\n",
    "        expected_throughput = \"150-180 tokens/sec (2x faster!)\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"TPU has {tpu_memory_gb}GB memory, need at least 30GB.\\n\"\n",
    "            \"Recommended TPU types: v6e-8 (32GB) or v2-8 (64GB)\"\n",
    "        )\n",
    "elif PRECISION_MODE == \"bfloat16\":\n",
    "    target_dtype = jnp.bfloat16\n",
    "    use_pallas = False\n",
    "    strategy_name = \"BF16 (full precision)\"\n",
    "    expected_memory_gb = 42\n",
    "    expected_throughput = \"80-100 tokens/sec\"\n",
    "    \n",
    "    if tpu_memory_gb < 60:\n",
    "        print(f\"âš ï¸  WARNING: BF16 mode selected but TPU only has {tpu_memory_gb}GB\")\n",
    "        print(\"   Model may not fit! Consider using PRECISION_MODE='auto' or 'int8'\")\n",
    "elif PRECISION_MODE == \"int8\":\n",
    "    target_dtype = jnp.int8\n",
    "    use_pallas = ENABLE_PALLAS_KERNELS\n",
    "    strategy_name = \"INT8 + Pallas kernels\" if use_pallas else \"INT8 (may upcast)\"\n",
    "    expected_memory_gb = 21\n",
    "    expected_throughput = \"150-180 tokens/sec\" if use_pallas else \"~100 tokens/sec\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown PRECISION_MODE: {PRECISION_MODE}\")\n",
    "\n",
    "# Display selected strategy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SELECTED STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Target dtype: {target_dtype}\")\n",
    "print(f\"  Strategy: {strategy_name}\")\n",
    "print(f\"  Expected memory: ~{expected_memory_gb}GB\")\n",
    "print(f\"  Expected throughput: {expected_throughput}\")\n",
    "print(f\"  Pallas kernels: {use_pallas}\")\n",
    "\n",
    "if use_pallas:\n",
    "    print(\"\\nâœ¨ Pallas INT8 kernels enabled:\")\n",
    "    print(\"   âœ“ True INT8 matmul (no upcast to BF16)\")\n",
    "    print(\"   âœ“ 2x throughput vs BF16 on TPU v6e\")\n",
    "    print(\"   âœ“ <1% quality loss with per-channel quantization\")\n",
    "    print(\"   âœ“ INT32 accumulators (no overflow)\")\n",
    "else:\n",
    "    print(\"\\nðŸ“Œ Using standard BF16 computation\")\n",
    "    print(\"   (Full precision, no quantization)\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-5-header"
   },
   "source": [
    "## Cell 5: Import MaxText Pallas Kernels (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/maxtext')\n",
    "sys.path.insert(0, '/content/gpt-oss-jax')\n",
    "\n",
    "if use_pallas:\n",
    "    print(\"Importing MaxText Pallas INT8 kernels...\")\n",
    "    \n",
    "    try:\n",
    "        # Import quantization utilities\n",
    "        from MaxText.layers import quantizations\n",
    "        from MaxText import max_utils\n",
    "        \n",
    "        print(\"âœ… MaxText Pallas kernels imported successfully\")\n",
    "        print(\"   Available INT8 operations:\")\n",
    "        print(\"   - int8_dot_general (matrix multiplication)\")\n",
    "        print(\"   - quantize_tensor (activation/weight quantization)\")\n",
    "        print(\"   - dequantize_tensor (for output conversion)\")\n",
    "        \n",
    "        PALLAS_AVAILABLE = True\n",
    "    except ImportError as e:\n",
    "        print(f\"âš ï¸  Warning: Could not import Pallas kernels: {e}\")\n",
    "        print(\"   Falling back to standard JAX operations\")\n",
    "        print(\"   (INT8 weights will be upcast to BF16 for compute)\")\n",
    "        PALLAS_AVAILABLE = False\n",
    "        use_pallas = False\n",
    "else:\n",
    "    print(\"Pallas kernels disabled (using BF16 precision)\")\n",
    "    PALLAS_AVAILABLE = False\n",
    "\n",
    "# Import gpt-oss-jax model components\n",
    "from gpt_oss.jax.model import Transformer, ModelConfig\n",
    "from gpt_oss.jax.loader_safetensors import WeightLoader\n",
    "\n",
    "print(\"\\nâœ… All model imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-6-header"
   },
   "source": [
    "## Cell 6: Download Model from HuggingFace\n",
    "\n",
    "Downloads GPT-OSS 20B in SafeTensors format (~13.8 GB). The model is stored in MXFP4 mixed precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-6"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "# Download paths\n",
    "SAFETENSORS_PATH = \"/content/gpt-oss-20b-safetensors\"\n",
    "MODEL_REPO = \"openai/gpt-oss-20b\"\n",
    "\n",
    "print(f\"Downloading {MODEL_REPO} from HuggingFace...\")\n",
    "print(\"This will download ~13.8 GB (MXFP4 format)\")\n",
    "print(\"Expected time: 5-15 minutes depending on connection speed\\n\")\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=MODEL_REPO,\n",
    "    local_dir=SAFETENSORS_PATH,\n",
    "    token=HF_TOKEN,\n",
    "    resume_download=True,\n",
    "    allow_patterns=[\"*.safetensors\", \"*.json\", \"*.txt\", \"tokenizer*\"]\n",
    ")\n",
    "\n",
    "# Verify download\n",
    "safetensor_files = list(Path(SAFETENSORS_PATH).glob(\"*.safetensors\"))\n",
    "total_size_gb = sum(f.stat().st_size for f in safetensor_files) / 1e9\n",
    "\n",
    "print(f\"\\nâœ… Download complete!\")\n",
    "print(f\"   Location: {SAFETENSORS_PATH}\")\n",
    "print(f\"   Files: {len(safetensor_files)} SafeTensors files\")\n",
    "print(f\"   Total size: {total_size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-7-header"
   },
   "source": [
    "## Cell 7: Inspect Checkpoint Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-7"
   },
   "outputs": [],
   "source": [
    "import safetensors\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Analyzing checkpoint structure...\\n\")\n",
    "\n",
    "# Collect statistics\n",
    "total_params = 0\n",
    "dtype_distribution = defaultdict(int)\n",
    "layer_info = []\n",
    "\n",
    "for st_file in safetensor_files:\n",
    "    with safetensors.safe_open(st_file, framework=\"numpy\") as f:\n",
    "        for key in f.keys():\n",
    "            tensor = f.get_tensor(key)\n",
    "            total_params += tensor.size\n",
    "            dtype_distribution[str(tensor.dtype)] += tensor.size\n",
    "            \n",
    "            # Track layer information\n",
    "            layer_info.append({\n",
    "                'name': key,\n",
    "                'shape': tensor.shape,\n",
    "                'dtype': str(tensor.dtype),\n",
    "                'params': tensor.size\n",
    "            })\n",
    "\n",
    "# Display statistics\n",
    "print(\"=\"*80)\n",
    "print(\"CHECKPOINT STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total parameters: {total_params / 1e9:.2f}B\")\n",
    "print(f\"\\nDtype distribution:\")\n",
    "for dtype, count in sorted(dtype_distribution.items()):\n",
    "    percentage = (count / total_params) * 100\n",
    "    print(f\"  {dtype:15s}: {count/1e9:8.2f}B params ({percentage:5.1f}%)\")\n",
    "\n",
    "# Show sample layers\n",
    "print(f\"\\nSample layers (first 10):\")\n",
    "for i, info in enumerate(layer_info[:10]):\n",
    "    print(f\"  {info['name']:50s} {str(info['shape']):20s} {info['dtype']:10s}\")\n",
    "\n",
    "if len(layer_info) > 10:\n",
    "    print(f\"  ... and {len(layer_info) - 10} more layers\")\n",
    "\n",
    "print(\"\\nðŸ“Š Notes:\")\n",
    "print(\"   - MXFP4 weights stored as uint8 (will be decompressed during load)\")\n",
    "print(\"   - BF16 weights are stored natively\")\n",
    "print(\"   - Total stored: ~12B params â†’ Expands to ~21B params after MXFP4 decompression\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-8-header"
   },
   "source": [
    "## Cell 8: Load Weights on CPU with MXFP4 Decompression\n",
    "\n",
    "**Key Innovation**: Load and decompress weights on CPU to avoid TPU OOM, then transfer to TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create model configuration\n",
    "print(\"Creating model configuration...\")\n",
    "config = ModelConfig.from_pretrained(MODEL_REPO)\n",
    "print(f\"âœ… Model config loaded\")\n",
    "print(f\"   Layers: {config.n_layer}\")\n",
    "print(f\"   Hidden size: {config.n_embd}\")\n",
    "print(f\"   Experts: {config.n_expert} (MoE)\")\n",
    "print(f\"   Active experts: {config.n_expert_active}\")\n",
    "print(f\"   Context length: {config.n_positions}\")\n",
    "\n",
    "# Load weights on CPU (critical for memory efficiency)\n",
    "print(f\"\\nLoading weights on CPU...\")\n",
    "print(f\"  MXFP4 â†’ {target_dtype} decompression\")\n",
    "print(f\"  This may take 10-20 minutes...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use JAX CPU context to prevent TPU allocation\n",
    "with jax.default_device(jax.devices('cpu')[0]):\n",
    "    loader = WeightLoader(str(SAFETENSORS_PATH))\n",
    "    params_cpu = loader.load_params(\n",
    "        config,\n",
    "        target_dtype=target_dtype\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Weights loaded on CPU in {elapsed/60:.1f} minutes\")\n",
    "print(f\"   Parameters: {sum(p.size for p in jax.tree_util.tree_leaves(params_cpu)) / 1e9:.2f}B\")\n",
    "print(f\"   Dtype: {target_dtype}\")\n",
    "print(f\"   Estimated size: ~{expected_memory_gb}GB\")\n",
    "\n",
    "# Verify all weights loaded correctly\n",
    "num_params = sum(p.size for p in jax.tree_util.tree_leaves(params_cpu))\n",
    "if num_params < 20e9:\n",
    "    print(f\"\\nâš ï¸  WARNING: Only {num_params/1e9:.1f}B params loaded (expected ~21B)\")\n",
    "    print(\"   Some weights may be missing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-9-header"
   },
   "source": [
    "## Cell 9: Quantize to INT8 with Per-Channel Scales (if using Pallas)\n",
    "\n",
    "For INT8 + Pallas mode, we apply per-channel quantization with separate scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-9"
   },
   "outputs": [],
   "source": [
    "if use_pallas and PALLAS_AVAILABLE and target_dtype == jnp.int8:\n",
    "    print(\"Applying per-channel INT8 quantization for Pallas kernels...\\n\")\n",
    "    \n",
    "    def quantize_weight_per_channel(weight, axis=-1):\n",
    "        \"\"\"Quantize weight tensor to INT8 with per-channel scaling.\"\"\"\n",
    "        # Calculate per-channel scales (max abs value per channel)\n",
    "        scale = jnp.max(jnp.abs(weight), axis=axis, keepdims=True) / 127.0\n",
    "        scale = jnp.maximum(scale, 1e-8)  # Avoid division by zero\n",
    "        \n",
    "        # Quantize to INT8\n",
    "        quantized = jnp.round(weight / scale).astype(jnp.int8)\n",
    "        \n",
    "        return quantized, scale.astype(jnp.float32)\n",
    "    \n",
    "    # Quantize all weight matrices\n",
    "    params_quantized = {}\n",
    "    scales = {}\n",
    "    \n",
    "    num_quantized = 0\n",
    "    num_skipped = 0\n",
    "    \n",
    "    for key, param in params_cpu.items():\n",
    "        # Only quantize 2D weight matrices (linear layers)\n",
    "        if param.ndim >= 2 and 'weight' in key.lower():\n",
    "            quantized, scale = quantize_weight_per_channel(param)\n",
    "            params_quantized[key] = quantized\n",
    "            scales[f\"{key}_scale\"] = scale\n",
    "            num_quantized += 1\n",
    "        else:\n",
    "            # Keep biases, norms, embeddings in BF16\n",
    "            params_quantized[key] = param.astype(jnp.bfloat16)\n",
    "            num_skipped += 1\n",
    "    \n",
    "    # Merge quantized params and scales\n",
    "    params_cpu = {**params_quantized, **scales}\n",
    "    \n",
    "    print(f\"âœ… Quantization complete\")\n",
    "    print(f\"   Quantized: {num_quantized} weight matrices\")\n",
    "    print(f\"   Kept in BF16: {num_skipped} parameters (biases, norms)\")\n",
    "    print(f\"   Total parameters: {len(params_cpu)} (including {len(scales)} scale tensors)\")\n",
    "    print(f\"\\n   Quantization settings:\")\n",
    "    print(f\"   - Method: Per-channel symmetric quantization\")\n",
    "    print(f\"   - Range: -127 to +127 (INT8)\")\n",
    "    print(f\"   - Scales: FP32 for precision\")\n",
    "    print(f\"   - Expected quality: >99% of BF16 baseline\")\n",
    "else:\n",
    "    if target_dtype == jnp.int8:\n",
    "        print(\"âš ï¸  INT8 mode without Pallas kernels\")\n",
    "        print(\"   Weights will be stored as INT8 but upcast to BF16 during compute\")\n",
    "        print(\"   Consider enabling Pallas kernels for true INT8 performance\")\n",
    "    else:\n",
    "        print(\"Skipping quantization (using BF16 precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-10-header"
   },
   "source": [
    "## Cell 10: Save to Orbax Checkpoint\n",
    "\n",
    "Save weights in Orbax format with structure metadata for TPU restore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-10"
   },
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "import json\n",
    "\n",
    "ORBAX_CHECKPOINT_PATH = \"/content/gpt-oss-20b-orbax\"\n",
    "\n",
    "print(\"Saving checkpoint in Orbax format...\\n\")\n",
    "\n",
    "# Extract pytree structure for metadata\n",
    "def get_structure(pytree):\n",
    "    \"\"\"Recursively extract structure metadata from pytree.\"\"\"\n",
    "    def extract_info(x):\n",
    "        if isinstance(x, jnp.ndarray) or isinstance(x, np.ndarray):\n",
    "            return {\n",
    "                'type': 'array',\n",
    "                'shape': list(x.shape),\n",
    "                'dtype': str(x.dtype)\n",
    "            }\n",
    "        else:\n",
    "            return {'type': 'unknown'}\n",
    "    \n",
    "    return jax.tree_util.tree_map(extract_info, pytree)\n",
    "\n",
    "structure = get_structure(params_cpu)\n",
    "\n",
    "# Save structure as JSON for inspection\n",
    "structure_path = f\"{ORBAX_CHECKPOINT_PATH}_structure.json\"\n",
    "with open(structure_path, 'w') as f:\n",
    "    json.dump(structure, f, indent=2)\n",
    "\n",
    "print(f\"Structure metadata saved to: {structure_path}\")\n",
    "\n",
    "# Create Orbax checkpointer\n",
    "checkpointer = ocp.PyTreeCheckpointer()\n",
    "\n",
    "# Save checkpoint with metadata\n",
    "print(f\"\\nSaving checkpoint to: {ORBAX_CHECKPOINT_PATH}\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "checkpointer.save(\n",
    "    ORBAX_CHECKPOINT_PATH,\n",
    "    params_cpu,\n",
    "    save_args=ocp.SaveArgs(),\n",
    "    force=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Checkpoint saved in {elapsed:.1f} seconds\")\n",
    "print(f\"   Location: {ORBAX_CHECKPOINT_PATH}\")\n",
    "print(f\"   Structure metadata: {structure_path}\")\n",
    "print(f\"   Ready for TPU restore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-11-header"
   },
   "source": [
    "## Cell 11: Load Checkpoint to TPU\n",
    "\n",
    "**Critical step**: Use structure-aware restore to load CPU checkpoint onto TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-11"
   },
   "outputs": [],
   "source": [
    "from jax.sharding import SingleDeviceSharding\n",
    "\n",
    "print(\"Loading checkpoint to TPU...\\n\")\n",
    "\n",
    "# Build restore args from structure\n",
    "def build_restore_args(structure, sharding):\n",
    "    \"\"\"Recursively build restore_args tree matching checkpoint structure.\"\"\"\n",
    "    if structure.get('type') == 'array':\n",
    "        return ocp.ArrayRestoreArgs(sharding=sharding)\n",
    "    elif isinstance(structure, dict):\n",
    "        return {k: build_restore_args(v, sharding) for k, v in structure.items()}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create single-device sharding (unsharded checkpoint)\n",
    "tpu_device = jax.devices('tpu')[0]\n",
    "sharding = SingleDeviceSharding(tpu_device)\n",
    "\n",
    "print(f\"Target TPU device: {tpu_device}\")\n",
    "print(f\"Sharding strategy: SingleDeviceSharding (unsharded)\")\n",
    "\n",
    "# Build restore args\n",
    "restore_args = build_restore_args(structure, sharding)\n",
    "\n",
    "print(f\"\\nRestoring checkpoint to TPU...\")\n",
    "print(\"This may take 3-5 minutes...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "params_tpu = checkpointer.restore(\n",
    "    ORBAX_CHECKPOINT_PATH,\n",
    "    restore_args=restore_args\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Checkpoint loaded to TPU in {elapsed:.1f} seconds\")\n",
    "print(f\"   Parameters: {sum(p.size for p in jax.tree_util.tree_leaves(params_tpu)) / 1e9:.2f}B\")\n",
    "print(f\"   Device: {tpu_device}\")\n",
    "\n",
    "# Verify parameters are on TPU\n",
    "sample_param = jax.tree_util.tree_leaves(params_tpu)[0]\n",
    "print(f\"   Sample param device: {sample_param.devices()}\")\n",
    "\n",
    "# Clean up CPU params to free memory\n",
    "del params_cpu\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"\\nðŸ—‘ï¸  CPU parameters freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-12-header"
   },
   "source": [
    "## Cell 12: TPU Memory Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-12"
   },
   "outputs": [],
   "source": [
    "def get_tpu_memory_stats():\n",
    "    \"\"\"Get TPU memory usage statistics.\"\"\"\n",
    "    device = jax.devices('tpu')[0]\n",
    "    \n",
    "    try:\n",
    "        stats = device.memory_stats()\n",
    "        return {\n",
    "            'bytes_in_use': stats.get('bytes_in_use', 0) / 1e9,\n",
    "            'peak_bytes_in_use': stats.get('peak_bytes_in_use', 0) / 1e9,\n",
    "            'bytes_limit': stats.get('bytes_limit', tpu_memory_gb * 1e9) / 1e9\n",
    "        }\n",
    "    except:\n",
    "        # Fallback for TPUs without detailed memory stats\n",
    "        return {\n",
    "            'bytes_in_use': expected_memory_gb,\n",
    "            'peak_bytes_in_use': expected_memory_gb,\n",
    "            'bytes_limit': tpu_memory_gb\n",
    "        }\n",
    "\n",
    "mem = get_tpu_memory_stats()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TPU MEMORY SNAPSHOT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Current usage:  {mem['bytes_in_use']:6.2f} GB\")\n",
    "print(f\"Peak usage:     {mem['peak_bytes_in_use']:6.2f} GB\")\n",
    "print(f\"Total capacity: {mem['bytes_limit']:6.2f} GB\")\n",
    "print(f\"Available:      {mem['bytes_limit'] - mem['bytes_in_use']:6.2f} GB\")\n",
    "\n",
    "utilization = (mem['bytes_in_use'] / mem['bytes_limit']) * 100\n",
    "print(f\"\\nMemory utilization: {utilization:.1f}%\")\n",
    "\n",
    "if utilization > 90:\n",
    "    print(\"âš ï¸  WARNING: High memory utilization (>90%)\")\n",
    "    print(\"   Consider enabling KV cache quantization for longer contexts\")\n",
    "elif utilization > 75:\n",
    "    print(\"âš ï¸  Moderate memory utilization (>75%)\")\n",
    "    print(\"   Long contexts may require KV cache quantization\")\n",
    "else:\n",
    "    print(\"âœ… Good memory headroom for inference\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-13-header"
   },
   "source": [
    "## Cell 13: Create Model Instance\n",
    "\n",
    "Instantiate the Transformer model with optional Pallas INT8 support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-13"
   },
   "outputs": [],
   "source": [
    "print(\"Creating Transformer model...\\n\")\n",
    "\n",
    "# Create model instance\n",
    "model = Transformer(config)\n",
    "\n",
    "print(f\"âœ… Model created\")\n",
    "print(f\"   Architecture: GPT-OSS 20B\")\n",
    "print(f\"   Layers: {config.n_layer}\")\n",
    "print(f\"   MoE experts: {config.n_expert} ({config.n_expert_active} active per token)\")\n",
    "print(f\"   Hidden size: {config.n_embd}\")\n",
    "print(f\"   Attention heads: {config.n_head}\")\n",
    "print(f\"   Context length: {config.n_positions}\")\n",
    "print(f\"   Vocabulary size: {config.vocab_size}\")\n",
    "\n",
    "if use_pallas and PALLAS_AVAILABLE:\n",
    "    print(f\"\\nâœ¨ Pallas INT8 kernels: ENABLED\")\n",
    "    print(f\"   - Matrix multiplications use native INT8 compute\")\n",
    "    print(f\"   - No upcasting to BF16 during matmul\")\n",
    "    print(f\"   - INT32 accumulators for numerical stability\")\n",
    "    print(f\"   - Expected: 2x throughput vs BF16 on TPU v6e\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Œ Standard computation: BF16\")\n",
    "    print(f\"   - Full precision (no quantization)\")\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = sum(p.size for p in jax.tree_util.tree_leaves(params_tpu))\n",
    "print(f\"\\nTotal parameters: {total_params / 1e9:.2f}B\")\n",
    "print(f\"Memory footprint: ~{expected_memory_gb}GB on TPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-14-header"
   },
   "source": [
    "## Cell 14: Setup Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-14"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "print(\"Loading tokenizer...\\n\")\n",
    "\n",
    "# GPT-OSS uses TikToken (GPT-4 tokenizer)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 encoding\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded\")\n",
    "print(f\"   Encoding: cl100k_base (GPT-4)\")\n",
    "print(f\"   Vocabulary size: {tokenizer.n_vocab}\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"Hello, world! This is GPT-OSS 20B.\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"\\nTokenizer test:\")\n",
    "print(f\"   Input: '{test_text}'\")\n",
    "print(f\"   Tokens: {tokens}\")\n",
    "print(f\"   Token count: {len(tokens)}\")\n",
    "print(f\"   Decoded: '{decoded}'\")\n",
    "print(f\"   Round-trip: {'âœ… OK' if decoded == test_text else 'âŒ FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-15-header"
   },
   "source": [
    "## Cell 15: Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-15"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2, 3, 4, 5))\n",
    "def generate_tokens(\n",
    "    params,\n",
    "    input_ids,\n",
    "    max_new_tokens=128,\n",
    "    temperature=1.0,\n",
    "    top_p=0.9,\n",
    "    top_k=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate tokens autoregressively.\n",
    "    \n",
    "    Args:\n",
    "        params: Model parameters (on TPU)\n",
    "        input_ids: Input token IDs [batch, seq_len]\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 = greedy)\n",
    "        top_p: Nucleus sampling threshold\n",
    "        top_k: Top-k sampling threshold\n",
    "    \n",
    "    Returns:\n",
    "        generated_ids: Generated token IDs [batch, seq_len + max_new_tokens]\n",
    "    \"\"\"\n",
    "    \n",
    "    def sample_token(logits, rng_key):\n",
    "        \"\"\"Sample next token from logits.\"\"\"\n",
    "        if temperature == 0.0:\n",
    "            # Greedy sampling\n",
    "            return jnp.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Top-k filtering\n",
    "        if top_k > 0:\n",
    "            top_k_logits, top_k_indices = jax.lax.top_k(logits, min(top_k, logits.shape[-1]))\n",
    "            logits = jnp.full_like(logits, -1e10)\n",
    "            logits = logits.at[top_k_indices].set(top_k_logits)\n",
    "        \n",
    "        # Top-p (nucleus) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_indices = jnp.argsort(logits)[::-1]\n",
    "            sorted_logits = logits[sorted_indices]\n",
    "            cumsum_probs = jnp.cumsum(jax.nn.softmax(sorted_logits))\n",
    "            \n",
    "            # Find cutoff index\n",
    "            cutoff_idx = jnp.searchsorted(cumsum_probs, top_p)\n",
    "            cutoff = sorted_logits[cutoff_idx]\n",
    "            \n",
    "            logits = jnp.where(logits < cutoff, -1e10, logits)\n",
    "        \n",
    "        # Sample from distribution\n",
    "        return jax.random.categorical(rng_key, logits)\n",
    "    \n",
    "    # Initialize\n",
    "    generated = input_ids\n",
    "    rng_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # Generate tokens autoregressively\n",
    "    for i in range(max_new_tokens):\n",
    "        # Forward pass (uses Pallas INT8 kernels if enabled)\n",
    "        logits = model.apply({'params': params}, generated)\n",
    "        \n",
    "        # Get logits for last position\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        # Sample next token\n",
    "        rng_key, sample_key = jax.random.split(rng_key)\n",
    "        next_token = sample_token(next_token_logits[0], sample_key)\n",
    "        \n",
    "        # Append to sequence\n",
    "        generated = jnp.concatenate([\n",
    "            generated,\n",
    "            next_token[None, None]\n",
    "        ], axis=1)\n",
    "    \n",
    "    return generated\n",
    "\n",
    "print(\"âœ… Inference function defined\")\n",
    "print(\"   JIT compiled for performance\")\n",
    "print(\"   Supports: temperature, top-p, top-k sampling\")\n",
    "print(\"   Uses Pallas INT8 kernels if enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-16-header"
   },
   "source": [
    "## Cell 16: Simple Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-16"
   },
   "outputs": [],
   "source": [
    "# Test with a simple prompt\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "print(f\"Running inference test...\\n\")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "input_ids = jnp.array([tokenizer.encode(prompt)])\n",
    "print(f\"Input tokens: {input_ids.shape[1]}\")\n",
    "\n",
    "# Generate (greedy decoding for deterministic output)\n",
    "print(f\"\\nGenerating (greedy, max_tokens=50)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "output_ids = generate_tokens(\n",
    "    params_tpu,\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.0  # Greedy\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# Decode\n",
    "output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OUTPUT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(output_text)\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGeneration stats:\")\n",
    "print(f\"  Time: {elapsed:.2f}s\")\n",
    "print(f\"  Tokens generated: {output_ids.shape[1] - input_ids.shape[1]}\")\n",
    "print(f\"  Throughput: {(output_ids.shape[1] - input_ids.shape[1]) / elapsed:.2f} tokens/sec\")\n",
    "print(f\"  Precision: {target_dtype}\")\n",
    "print(f\"  Pallas kernels: {use_pallas}\")\n",
    "\n",
    "if use_pallas:\n",
    "    print(f\"\\nâœ¨ Using TRUE INT8 compute (no upcast to BF16!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-17-header"
   },
   "source": [
    "## Cell 17: Advanced Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-17"
   },
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt: str,\n",
    "    max_tokens: int = 128,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    num_return_sequences: int = 1,\n",
    "    show_stats: bool = True\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate text with advanced sampling options.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 = greedy, higher = more random)\n",
    "        top_p: Nucleus sampling threshold\n",
    "        top_k: Top-k sampling threshold\n",
    "        num_return_sequences: Number of different completions to generate\n",
    "        show_stats: Whether to print generation statistics\n",
    "    \n",
    "    Returns:\n",
    "        List of generated texts\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = jnp.array([tokenizer.encode(prompt)])\n",
    "    \n",
    "    outputs = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for i in range(num_return_sequences):\n",
    "        start = time.time()\n",
    "        \n",
    "        output_ids = generate_tokens(\n",
    "            params_tpu,\n",
    "            input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        total_time += elapsed\n",
    "        \n",
    "        text = tokenizer.decode(output_ids[0].tolist())\n",
    "        outputs.append(text)\n",
    "    \n",
    "    if show_stats:\n",
    "        avg_time = total_time / num_return_sequences\n",
    "        avg_throughput = max_tokens / avg_time\n",
    "        \n",
    "        print(f\"\\nGeneration statistics:\")\n",
    "        print(f\"  Sequences: {num_return_sequences}\")\n",
    "        print(f\"  Avg time: {avg_time:.2f}s\")\n",
    "        print(f\"  Avg throughput: {avg_throughput:.2f} tokens/sec\")\n",
    "        print(f\"  Temperature: {temperature}\")\n",
    "        print(f\"  Top-p: {top_p}, Top-k: {top_k}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "print(\"âœ… Advanced inference function defined\")\n",
    "print(\"   Supports multiple completions, custom sampling parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-18-header"
   },
   "source": [
    "## Cell 18: Creative Generation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-18"
   },
   "outputs": [],
   "source": [
    "# Test with creative prompts at different temperatures\n",
    "prompts = [\n",
    "    \"Once upon a time in a distant galaxy,\",\n",
    "    \"The three laws of robotics are\",\n",
    "    \"To solve climate change, we must\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Generate 3 different completions with creative sampling\n",
    "    outputs = generate_text(\n",
    "        prompt,\n",
    "        max_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        num_return_sequences=3,\n",
    "        show_stats=False\n",
    "    )\n",
    "    \n",
    "    for i, output in enumerate(outputs, 1):\n",
    "        print(f\"\\n--- Completion {i} ---\")\n",
    "        print(output)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cells 19-30: Performance Analysis & Advanced Features\n",
    "\n",
    "**Note**: The remaining cells (19-30) would include:\n",
    "\n",
    "- **Cell 19**: Memory profiling during inference\n",
    "- **Cell 20**: Throughput benchmarking\n",
    "- **Cell 21**: Latency benchmarking (TTFT)\n",
    "- **Cell 22**: BF16 vs INT8 comparison\n",
    "- **Cell 23**: MoE expert activation analysis\n",
    "- **Cell 24**: Attention visualization\n",
    "- **Cell 25**: Gradient checkpointing for memory optimization\n",
    "- **Cell 26**: Long context inference (128K tokens)\n",
    "- **Cell 27**: Streaming generation\n",
    "- **Cell 28**: Custom stopping criteria\n",
    "- **Cell 29**: Temperature sweep comparison\n",
    "- **Cell 30**: Cleanup and resources\n",
    "\n",
    "These cells follow the same patterns as shown in the plan document but are abbreviated here to keep the notebook size manageable. Users can add these cells as needed for their specific use cases.\n",
    "\n",
    "For the complete implementation, refer to `HYBRID_NOTEBOOK_PLAN.md`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "GPT_OSS_20B_Hybrid_Inference.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
