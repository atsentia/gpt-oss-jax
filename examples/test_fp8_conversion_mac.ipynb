{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FP8 Conversion Test - Mac M3 Ultra\n",
        "\n",
        "This notebook tests the FP8 conversion on Mac with RAM monitoring.\n",
        "\n",
        "**Requirements:**\n",
        "- Mac M3 Ultra with 96GB RAM\n",
        "- gpt-oss-20b downloaded locally\n",
        "- JAX, Orbax, and dependencies installed\n",
        "\n",
        "**Cells:**\n",
        "1. Cell 0: Setup (configure paths)\n",
        "2. Cell 1: Download check (skips if path already set)\n",
        "3. Cell 2: FP8 Conversion (with RAM monitoring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Local Mac M3 Ultra setup - simulate TPU v6e for FP8 conversion testing\n",
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from pathlib import Path\n",
        "\n",
        "# Force JAX to use CPU (Mac doesn't have TPU)\n",
        "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
        "\n",
        "# Pretend to be TPU v6e to trigger FP8 conversion\n",
        "# (This is just for testing the conversion logic)\n",
        "os.environ['COLAB_TPU_ADDR'] = 'fake-tpu-v6e'\n",
        "\n",
        "# Verify JAX is using CPU\n",
        "print(\"JAX devices:\", jax.devices())\n",
        "print(\"JAX backend:\", jax.default_backend())\n",
        "\n",
        "# Set strategy manually for local testing\n",
        "STRATEGY = \"fp8\"  # Test FP8 conversion\n",
        "DTYPE = jnp.float8_e4m3fn\n",
        "\n",
        "# Set local paths (Mac uses current directory, not /content/)\n",
        "# ⚠️ UPDATE THIS: Point to the directory containing .safetensors files\n",
        "# Common paths:\n",
        "#   - \"gpt-oss-20b/original\" (HuggingFace cache structure)\n",
        "#   - \"/Users/yourname/models/gpt-oss-20b/original\"\n",
        "#   - \"~/Downloads/gpt-oss-20b/original\"\n",
        "safetensors_path = Path(\"gpt-oss-20b/original\")  # ← UPDATE THIS PATH!\n",
        "orbax_path = f\"gpt-oss-20b-orbax-{STRATEGY}\"\n",
        "\n",
        "# Verify path exists and contains .safetensors files\n",
        "if not safetensors_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Path does not exist: {safetensors_path}\\n\"\n",
        "        f\"Please update the safetensors_path in Cell 0 to point to your gpt-oss-20b download\"\n",
        "    )\n",
        "\n",
        "st_files = list(safetensors_path.glob('*.safetensors'))\n",
        "if len(st_files) == 0:\n",
        "    # Try common subdirectories\n",
        "    for subdir in ['original', 'models', '']:\n",
        "        candidate = safetensors_path / subdir if subdir else safetensors_path\n",
        "        st_files = list(candidate.glob('*.safetensors'))\n",
        "        if st_files:\n",
        "            safetensors_path = candidate\n",
        "            break\n",
        "    \n",
        "    if len(st_files) == 0:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No .safetensors files found in: {safetensors_path}\\n\"\n",
        "            f\"Please update the safetensors_path in Cell 0 to the correct directory\\n\"\n",
        "            f\"Expected files like: model.safetensors or model-00001-of-00002.safetensors\"\n",
        "        )\n",
        "\n",
        "print(f\"\\n✓ Local Mac setup complete\")\n",
        "print(f\"  Strategy: {STRATEGY.upper()}\")\n",
        "print(f\"  Target dtype: {DTYPE}\")\n",
        "print(f\"  SafeTensors path: {safetensors_path}\")\n",
        "print(f\"  Found {len(st_files)} .safetensors file(s)\")\n",
        "print(f\"  Orbax output: {orbax_path}\")\n",
        "print(f\"\\n⚠️ Now run Cell 1 (Download check) then Cell 2 (Conversion)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Download Check (skips download on Mac)\n",
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "# Only download if not already set (e.g., from Cell 0 for Mac testing)\n",
        "if 'safetensors_path' not in globals():\n",
        "    print(\"Downloading GPT-OSS-20B (13.8 GB)...\")\n",
        "    checkpoint_dir = snapshot_download(\n",
        "        repo_id=\"openai/gpt-oss-20b\",\n",
        "        revision=\"main\",\n",
        "        allow_patterns=[\"original/*\"],\n",
        "        local_dir=\"/content/gpt-oss-20b-dl\",\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "    safetensors_path = Path(\"/content/gpt-oss-20b-dl/original\")\n",
        "    print(f\"✓ Downloaded: {safetensors_path}\")\n",
        "else:\n",
        "    print(f\"✓ Using existing SafeTensors path: {safetensors_path}\")\n",
        "    print(\"  (Set in Cell 0 - skipping download)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: FP8 Conversion with RAM Monitoring\n",
        "import time\n",
        "import numpy as np\n",
        "import psutil\n",
        "from gpt_oss.jax.config import ModelConfig\n",
        "from gpt_oss.jax.loader_safetensors import WeightLoader\n",
        "from orbax.checkpoint import PyTreeCheckpointer\n",
        "import orbax.checkpoint as ocp\n",
        "from safetensors import safe_open\n",
        "from pathlib import Path\n",
        "from flax import traverse_util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Helper function to track memory usage\n",
        "def get_ram_gb():\n",
        "    \"\"\"Get current RAM usage in GB.\"\"\"\n",
        "    return psutil.Process().memory_info().rss / 1024**3\n",
        "\n",
        "config = ModelConfig()\n",
        "# Set orbax_path if not already set (e.g., from Cell 0 for Mac testing)\n",
        "if 'orbax_path' not in globals():\n",
        "    orbax_path = f\"/content/gpt-oss-20b-orbax-{STRATEGY}\"\n",
        "\n",
        "print(f\"Converting to Orbax ({STRATEGY.upper()})...\")\n",
        "print(\"Loading tensors on CPU to avoid TPU OOM...\")\n",
        "ram_start = get_ram_gb()\n",
        "print(f\"Starting RAM: {ram_start:.2f} GB\")\n",
        "t0 = time.time()\n",
        "\n",
        "# For TPU v6e with FP8: Load on CPU, convert, then save\n",
        "# This avoids loading 42GB BF16 into 32GB TPU memory\n",
        "with jax.default_device(jax.devices('cpu')[0]):\n",
        "    loader = WeightLoader(str(safetensors_path))\n",
        "    # Loads all weights as BF16:\n",
        "    # - Regular weights: Loaded as BF16 directly from SafeTensors\n",
        "    # - MXFP4 MoE weights: Decompressed MXFP4 → BF16 (see loader_safetensors.py)\n",
        "    params = loader.load_params(config, show_progress=True)\n",
        "    ram_after_load = get_ram_gb()\n",
        "    print(f\"After loading BF16: {ram_after_load:.2f} GB (+{ram_after_load - ram_start:.2f} GB)\")\n",
        "    \n",
        "    # Convert BF16 → FP8 if using FP8 strategy (TPU v6e)\n",
        "    if STRATEGY == \"fp8\":\n",
        "        print(\"Converting BF16 → FP8 on CPU...\")\n",
        "        # Converts all BF16 tensors to FP8, leaves other dtypes unchanged\n",
        "        params = jax.tree_util.tree_map(\n",
        "            lambda x: x.astype(DTYPE) if x.dtype == jnp.bfloat16 else x,\n",
        "            params\n",
        "        )\n",
        "        ram_after_fp8 = get_ram_gb()\n",
        "        print(f\"After FP8 conversion: {ram_after_fp8:.2f} GB (+{ram_after_fp8 - ram_start:.2f} GB total)\")\n",
        "\n",
        "# Save to Orbax format (still on CPU)\n",
        "print(\"Saving to Orbax...\")\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "checkpointer.save(orbax_path, params, save_args=ocp.SaveArgs(aggregate=True))\n",
        "ram_peak = get_ram_gb()\n",
        "\n",
        "print(f\"✓ Converted in {time.time()-t0:.1f}s\")\n",
        "print(f\"  Orbax checkpoint: {orbax_path}\")\n",
        "print(f\"  Peak RAM: {ram_peak:.2f} GB (+{ram_peak - ram_start:.2f} GB)\")\n",
        "\n",
        "# Free CPU memory\n",
        "del params\n",
        "import gc\n",
        "gc.collect()\n",
        "ram_after_cleanup = get_ram_gb()\n",
        "print(f\"  After cleanup: {ram_after_cleanup:.2f} GB ({ram_peak - ram_after_cleanup:.2f} GB freed)\")\n",
        "\n",
        "# Note: On Mac we keep the SafeTensors (no /content/ to clean up)\n",
        "print(\"\\n✓ FP8 conversion complete!\")\n",
        "print(f\"\\nExpected RAM usage:\")\n",
        "print(f\"  - Starting: ~0.2 GB\")\n",
        "print(f\"  - After BF16 load: ~42-44 GB\")\n",
        "print(f\"  - After FP8 convert: ~21-23 GB\")\n",
        "print(f\"  - Peak: ~25-30 GB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
