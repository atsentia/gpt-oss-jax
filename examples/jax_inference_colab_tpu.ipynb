{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# GPT-OSS-20B on Google Colab TPU\n\n<span style=\"color: #e67e22; font-weight: bold;\">‚ö†Ô∏è Note: This is a basic non-optimized implementation for educational purposes.</span>\n\n## Adaptive Precision Inference\n\nRepository: [gpt-oss-jax](https://github.com/atsentia/gpt-oss-jax)\n\n### Adaptive Precision Strategy\n\n| TPU Type | Memory | Strategy | Model Size |\n|----------|--------|----------|------------|\n| **v2-8** | 64GB (8x8GB) | BF16 (16-bit) | ~42GB |\n| **v6e** | 32GB | FP8 (8-bit) | ~21GB |\n\n### ‚ö†Ô∏è Setup Required\n\n**Runtime ‚Üí Change runtime type ‚Üí TPU** (before running cells)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "This cell installs all required packages:\n",
    "- **JAX with TPU support** - Core ML framework optimized for TPUs\n",
    "- **Flax & Orbax** - Neural network library and checkpoint utilities\n",
    "- **openai-harmony** - Harmony protocol for multi-channel reasoning\n",
    "- **gpt-oss-jax** - Our GPT-OSS-20B implementation\n",
    "\n",
    "**Expected time**: ~2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q \"jax[tpu]>=0.4.20\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install -q flax orbax-checkpoint safetensors openai-harmony tiktoken tqdm huggingface_hub\n",
    "\n",
    "# Clone repo\n",
    "!git clone -q https://github.com/atsentia/gpt-oss-jax.git 2>/dev/null || true\n",
    "%cd gpt-oss-jax\n",
    "!pip install -q -e \".[jax]\"\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify TPU Backend & Select Precision Strategy\n",
    "\n",
    "This cell:\n",
    "1. **Detects your TPU type** (v2-8, v6e, etc.)\n",
    "2. **Validates TPU is available** (not CPU)\n",
    "3. **Automatically selects precision strategy**:\n",
    "   - TPU v2-8 with 8 devices ‚Üí **BF16** (16-bit, 64GB HBM)\n",
    "   - TPU v6e ‚Üí **FP8** (8-bit, 32GB HBM)\n",
    "\n",
    "**What to expect**: Should print your TPU type and selected strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Load Model Parameters with Adaptive Precision\n\nLoads the 21B parameters into memory using your selected precision strategy.\n\nBF16 Strategy (TPU v2-8):\n- Loads weights directly as bfloat16\n- Memory footprint: ~42GB\n- Best accuracy (full precision)\n\nFP8 Strategy (TPU v6e):\n- Loads weights and converts to float8_e4m3fn immediately\n- Memory footprint: ~21GB (50% reduction!)\n- Minimal accuracy loss (<2% perplexity increase)\n- Uses target_dtype parameter to avoid BF16 memory spike\n\nExpected time: ~5-10 seconds"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import time\nfrom gpt_oss.jax.config import ModelConfig\nfrom gpt_oss.jax.loader_safetensors import WeightLoader\n\nconfig = ModelConfig()\nprint(f\"Loading with {STRATEGY.upper()}...\")\nt0 = time.time()\n\nloader = WeightLoader(str(safetensors_path))\n\n# Use target_dtype parameter to load directly in target precision\n# This avoids BF16 memory spike on TPU v6e when using FP8\nparams = loader.load_params(config, show_progress=True, target_dtype=DTYPE)\n\nprint(f\"‚úì Loaded in {time.time()-t0:.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Save Checkpoint in Orbax Format (Optional)\n\nSaves the loaded parameters to Orbax format for faster loading in future sessions.\n\nWhy Orbax?\n- 2-3x faster loading than safetensors\n- Optimized for JAX PyTree structures\n- Supports sharded checkpoints across TPU devices\n\nYou can skip this cell if you don't need persistent checkpoints.\n\nExpected time: ~10-15 seconds"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "from orbax.checkpoint import PyTreeCheckpointer\nimport orbax.checkpoint as ocp\n\norbax_path = f\"/content/gpt-oss-20b-orbax-{STRATEGY}\"\nprint(f\"Saving Orbax ({STRATEGY})...\")\n\ncheckpointer = ocp.PyTreeCheckpointer()\ncheckpointer.save(orbax_path, params, save_args=ocp.SaveArgs(aggregate=True))\n\nprint(f\"‚úì Saved: {orbax_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Memory Utilization Analysis\n\nCalculates actual memory usage and compares to TPU HBM capacity.\n\nWhat you'll see:\n- Actual memory: Size of loaded parameters in GB\n- TPU HBM: Total high-bandwidth memory available\n- Utilization: Percentage of HBM used\n\nTarget utilization: ~66% (leaves headroom for activations and KV cache)\n\nIf you see >90% utilization: Model may not fit for inference"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def mem_gb(p):\n    return sum(x.nbytes for x in jax.tree_util.tree_leaves(p)) / 1e9\n\nactual = mem_gb(params)\nprint(f\"Memory: {actual:.1f} GB (expected: {MEM_GB} GB)\")\n\ntpu_hbm = 64 if \"v2\" in tpu_type and num_devices == 8 else 32\nprint(f\"TPU HBM: {tpu_hbm} GB ({actual/tpu_hbm*100:.0f}% used)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8. Run Inference with Harmony Protocol\n\nDemonstrates multi-channel reasoning using the Harmony protocol.\n\nHarmony Protocol Features:\n- Multi-channel output: Separate analysis and final answer channels\n- Structured reasoning: Model shows its thought process\n- Efficient inference: Uses KV cache for fast token generation\n\nExample Question: \"What is the capital of France?\"\n\nExpected output:\n- Analysis channel: Model's reasoning process (üìä)\n- Answer channel: Final response (üí¨)\n- Performance: Tokens/second metric\n\nTry it: Edit the msg variable to ask your own questions!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Model & Tokenizer\n",
    "\n",
    "Creates the GPT-OSS-20B Transformer model and tokenizer.\n",
    "\n",
    "**What happens**:\n",
    "- Initializes the model architecture (40 layers, 8192 hidden dim, 64 attention heads)\n",
    "- Loads the tokenizer (GPT-2 style BPE with 50,257 tokens)\n",
    "- Verifies parameter dtype matches your strategy\n",
    "\n",
    "**Model Architecture**:\n",
    "- Parameters: 20.8B\n",
    "- Layers: 40\n",
    "- Context: 8192 tokens\n",
    "- Vocab: 50,257 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 9. Optional: Save to Google Drive\n\nUncomment the code below to save your Orbax checkpoint to Google Drive.\n\nWhy save to Drive?\n- Colab sessions are temporary (max 12 hours)\n- Avoid re-downloading model weights in future sessions\n- 2-3x faster loading from Drive than HuggingFace\n\nNote: Requires ~20-42 GB of Drive storage depending on precision strategy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Utilization Analysis\n",
    "\n",
    "Calculates actual memory usage and compares to TPU HBM capacity.\n",
    "\n",
    "**What you'll see**:\n",
    "- **Actual memory**: Size of loaded parameters in GB\n",
    "- **TPU HBM**: Total high-bandwidth memory available\n",
    "- **Utilization**: Percentage of HBM used\n",
    "\n",
    "**Target utilization**: ~66% (leaves headroom for activations and KV cache)\n",
    "\n",
    "**If you see >90% utilization**: Model may not fit for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 10. TPU Memory Monitoring\n\nReal-time monitoring of TPU memory usage across all devices.\n\nWhat you'll see:\n- Per-device breakdown: Memory usage for each TPU core\n- Bytes in use: Current memory consumption\n- Bytes limit: Maximum available memory\n- Utilization percentage: How much of each device's memory is used\n\nUse this to:\n- Debug OOM (Out of Memory) errors\n- Verify memory is balanced across devices\n- Monitor memory during inference"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference with Harmony Protocol\n",
    "\n",
    "Demonstrates multi-channel reasoning using the Harmony protocol.\n",
    "\n",
    "**Harmony Protocol Features**:\n",
    "- **Multi-channel output**: Separate analysis and final answer channels\n",
    "- **Structured reasoning**: Model shows its thought process\n",
    "- **Efficient inference**: Uses KV cache for fast token generation\n",
    "\n",
    "**Example Question**: \"What is the capital of France?\"\n",
    "\n",
    "**Expected output**:\n",
    "- üìä **Analysis channel**: Model's reasoning process\n",
    "- üí¨ **Answer channel**: Final response\n",
    "- **Performance**: Tokens/second metric\n",
    "\n",
    "**Try it**: Edit the `msg` variable to ask your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 11. Cleanup Temporary Files\n\nRemoves the temporary download directory to free up disk space.\n\nWhat gets deleted:\n- /content/gpt-oss-20b-dl/ (13.8 GB)\n- Original safetensors files\n\nWhat's preserved:\n- Loaded parameters in memory\n- Orbax checkpoint (if you ran Cell 5)\n\nSafe to run: Parameters are already loaded in RAM"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "| Metric | TPU v2-8 (BF16) | TPU v6e (FP8) |\n",
    "|--------|-----------------|---------------|\n",
    "| Precision | 16-bit | 8-bit |\n",
    "| Memory | ~42 GB | ~21 GB |\n",
    "| TPU HBM | 64 GB | 32 GB |\n",
    "| Utilization | 66% | 66% |\n",
    "| Load Time | ~5s | ~5s |\n",
    "| Tokens/sec | 50-100 | 80-150* |\n",
    "\n",
    "\\* FP8 may be faster due to lower memory bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optional: Save to Google Drive\n",
    "\n",
    "Uncomment the code below to save your Orbax checkpoint to Google Drive.\n",
    "\n",
    "**Why save to Drive?**\n",
    "- Colab sessions are temporary (max 12 hours)\n",
    "- Avoid re-downloading model weights in future sessions\n",
    "- 2-3x faster loading from Drive than HuggingFace\n",
    "\n",
    "**Note**: Requires ~20-42 GB of Drive storage depending on precision strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r {orbax_path} /content/drive/MyDrive/\n",
    "# print(\"‚úì Saved to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. TPU Memory Monitoring\n",
    "\n",
    "Real-time monitoring of TPU memory usage across all devices.\n",
    "\n",
    "**What you'll see**:\n",
    "- **Per-device breakdown**: Memory usage for each TPU core\n",
    "- **Bytes in use**: Current memory consumption\n",
    "- **Bytes limit**: Maximum available memory\n",
    "- **Utilization percentage**: How much of each device's memory is used\n",
    "\n",
    "**Use this to**:\n",
    "- Debug OOM (Out of Memory) errors\n",
    "- Verify memory is balanced across devices\n",
    "- Monitor memory during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TPU Monitoring:\")\n",
    "try:\n",
    "    from jax.lib import xla_bridge\n",
    "    backend = xla_bridge.get_backend()\n",
    "    for i, dev in enumerate(devices):\n",
    "        try:\n",
    "            info = backend.get_memory_info(dev)\n",
    "            if info:\n",
    "                used = info.bytes_in_use / 1e9\n",
    "                limit = info.bytes_limit / 1e9\n",
    "                print(f\"  Device {i}: {used:.1f}/{limit:.1f} GB ({used/limit*100:.0f}%)\")\n",
    "        except:\n",
    "            print(f\"  Device {i}: Memory info unavailable\")\n",
    "except Exception as e:\n",
    "    print(f\"  Monitoring unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup Temporary Files\n",
    "\n",
    "Removes the temporary download directory to free up disk space.\n",
    "\n",
    "**What gets deleted**:\n",
    "- `/content/gpt-oss-20b-dl/` (13.8 GB)\n",
    "- Original safetensors files\n",
    "\n",
    "**What's preserved**:\n",
    "- Loaded parameters in memory\n",
    "- Orbax checkpoint (if you ran Cell 5)\n",
    "\n",
    "**Safe to run**: Parameters are already loaded in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temp files\n",
    "!rm -rf /content/gpt-oss-20b-dl\n",
    "print(\"‚úì Cleaned temp files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**OOM Errors**: Verify TPU type matches strategy (Cell 3)\n",
    "\n",
    "**TPU Not Detected**: Runtime ‚Üí Change runtime type ‚Üí TPU, then restart\n",
    "\n",
    "**Slow Download**: HuggingFace rate limits - wait and retry\n",
    "\n",
    "**Import Errors**: Re-run Cell 2 (environment setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Optimization Exercises\n",
    "\n",
    "### 1. JAX Code Optimization\n",
    "Profile with `jax.profiler`, optimize bottlenecks\n",
    "\n",
    "[Code](https://github.com/atsentia/gpt-oss-jax/blob/main/gpt_oss/jax/model.py)\n",
    "\n",
    "### 2. KV Cache Optimization\n",
    "Implement INT8/FP8 KV cache for 2-4x memory savings\n",
    "\n",
    "[Code](https://github.com/atsentia/gpt-oss-jax/blob/main/gpt_oss/jax/kv_cache.py)\n",
    "\n",
    "### 3. Advanced Quantization\n",
    "On-the-fly MXFP4 dequantization: 10.5 GB vs 21 GB\n",
    "\n",
    "[Code](https://github.com/atsentia/gpt-oss-jax/tree/main/gpt_oss/jax/quantization)\n",
    "\n",
    "### 4. Speculative Decoding\n",
    "Draft model (GPT-2) + verification: 2-3x speedup\n",
    "\n",
    "### 5. Continuous Batching\n",
    "Batch multiple requests: 5-10x throughput\n",
    "\n",
    "**Discuss**: [GitHub Discussions](https://github.com/atsentia/gpt-oss-jax/discussions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "‚úÖ Demonstrated adaptive precision (BF16 vs FP8)\n",
    "\n",
    "‚úÖ 2x memory reduction enables TPU v6e\n",
    "\n",
    "‚úÖ Production patterns: monitoring, error handling\n",
    "\n",
    "‚úÖ Harmony protocol multi-channel reasoning\n",
    "\n",
    "### Resources\n",
    "- [Repository](https://github.com/atsentia/gpt-oss-jax)\n",
    "- [Model Card](https://huggingface.co/openai/gpt-oss-20b)\n",
    "- [JAX Docs](https://jax.readthedocs.io/)\n",
    "\n",
    "**Issues?** [Open an issue](https://github.com/atsentia/gpt-oss-jax/issues)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}