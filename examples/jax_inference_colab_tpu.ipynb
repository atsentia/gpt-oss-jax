{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Local Mac Testing Setup (Optional)\n",
        "\n",
        "**\u26a0\ufe0f Skip this cell if running on Colab TPU**\n",
        "\n",
        "This cell configures JAX to simulate TPU v6e behavior on Mac M3 Ultra for testing the FP8 conversion:\n",
        "- Pretends to be TPU v6e (triggers FP8 strategy)\n",
        "- Forces CPU execution for conversion (same as Colab)\n",
        "- Useful for testing locally with 96GB Mac RAM before deploying to Colab\n",
        "\n",
        "After running this, skip cells 1-2 (dependencies) and go directly to cell 3 (download HuggingFace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local Mac M3 Ultra setup - simulate TPU v6e for FP8 conversion testing\n",
        "import os\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from pathlib import Path\n",
        "\n",
        "# Force JAX to use CPU (Mac doesn't have TPU)\n",
        "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
        "\n",
        "# Pretend to be TPU v6e to trigger FP8 conversion\n",
        "# (This is just for testing the conversion logic)\n",
        "os.environ['COLAB_TPU_ADDR'] = 'fake-tpu-v6e'\n",
        "\n",
        "# Verify JAX is using CPU\n",
        "print(\"JAX devices:\", jax.devices())\n",
        "print(\"JAX backend:\", jax.default_backend())\n",
        "\n",
        "# Set strategy manually for local testing\n",
        "STRATEGY = \"fp8\"  # Test FP8 conversion\n",
        "DTYPE = jnp.float8_e4m3fn\n",
        "\n",
        "# Set local paths (Mac uses current directory, not /content/)\n",
        "# \u26a0\ufe0f UPDATE THIS: Point to the directory containing .safetensors files\n",
        "# Common paths:\n",
        "#   - \"gpt-oss-20b/original\" (HuggingFace cache structure)\n",
        "#   - \"/Users/yourname/models/gpt-oss-20b/original\"\n",
        "#   - \"~/Downloads/gpt-oss-20b/original\"\n",
        "safetensors_path = Path(\"gpt-oss-20b/original\")  # \u2190 UPDATE THIS PATH!\n",
        "orbax_path = f\"gpt-oss-20b-orbax-{STRATEGY}\"\n",
        "\n",
        "# Verify path exists and contains .safetensors files\n",
        "if not safetensors_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Path does not exist: {safetensors_path}\\n\"\n",
        "        f\"Please update the safetensors_path in Cell 0 to point to your gpt-oss-20b download\"\n",
        "    )\n",
        "\n",
        "st_files = list(safetensors_path.glob('*.safetensors'))\n",
        "if len(st_files) == 0:\n",
        "    # Try common subdirectories\n",
        "    for subdir in ['original', 'models', '']:\n",
        "        candidate = safetensors_path / subdir if subdir else safetensors_path\n",
        "        st_files = list(candidate.glob('*.safetensors'))\n",
        "        if st_files:\n",
        "            safetensors_path = candidate\n",
        "            break\n",
        "    \n",
        "    if len(st_files) == 0:\n",
        "        raise FileNotFoundError(\n",
        "            f\"No .safetensors files found in: {safetensors_path}\\n\"\n",
        "            f\"Please update the safetensors_path in Cell 0 to the correct directory\\n\"\n",
        "            f\"Expected files like: model.safetensors or model-00001-of-00002.safetensors\"\n",
        "        )\n",
        "\n",
        "print(f\"\\n\u2713 Local Mac setup complete\")\n",
        "print(f\"  Strategy: {STRATEGY.upper()}\")\n",
        "print(f\"  Target dtype: {DTYPE}\")\n",
        "print(f\"  SafeTensors path: {safetensors_path}\")\n",
        "print(f\"  Found {len(st_files)} .safetensors file(s)\")\n",
        "print(f\"  Orbax output: {orbax_path}\")\n",
        "print(f\"\\n\u26a0\ufe0f Now skip cells 1-4 and run cell 5 (Download from HuggingFace)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JAX: GPT-OSS-20B on Google Colab TPU\n",
        "\n",
        "<span style=\"color: #e67e22; font-weight: bold;\">\u26a0\ufe0f Note: This is a basic non-optimized implementation for educational purposes.</span>\n",
        "\n",
        "## Adaptive Precision Inference\n",
        "\n",
        "Repository: [gpt-oss-jax](https://github.com/atsentia/gpt-oss-jax)\n",
        "\n",
        "### Adaptive Precision Strategy\n",
        "\n",
        "| TPU Type | Memory | Strategy | Model Size |\n",
        "|----------|--------|----------|------------|\n",
        "| **v2-8** | 64GB (8x8GB) | BF16 (16-bit) | ~42GB |\n",
        "| **v6e** | 32GB | FP8 (8-bit) | ~21GB |\n",
        "\n",
        "### \u26a0\ufe0f Setup Required\n",
        "\n",
        "**Runtime \u2192 Change runtime type \u2192 TPU** (before running cells)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "This cell installs all required packages:\n",
        "- JAX with TPU support - Core ML framework optimized for TPUs\n",
        "- Flax & Orbax - Neural network library and checkpoint utilities\n",
        "- openai-harmony - Harmony protocol for multi-channel reasoning\n",
        "- gpt-oss-jax - Our GPT-OSS-20B implementation\n",
        "\n",
        "Expected time: ~2-3 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q \"jax[tpu]>=0.4.20\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -q flax orbax-checkpoint safetensors openai-harmony tiktoken tqdm huggingface_hub\n",
        "\n",
        "# Clone repo\n",
        "!git clone -q https://github.com/atsentia/gpt-oss-jax.git 2>/dev/null || true\n",
        "%cd gpt-oss-jax\n",
        "!pip install -q -e \".[jax]\"\n",
        "\n",
        "print(\"\u2713 Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify TPU Backend & Select Precision Strategy\n",
        "\n",
        "This cell:\n",
        "1. Detects your TPU type (v2-8, v6e, etc.)\n",
        "2. Validates TPU is available (not CPU)\n",
        "3. Automatically selects precision strategy:\n",
        "   - TPU v2-8 with 8 devices \u2192 BF16 (16-bit, 64GB HBM)\n",
        "   - TPU v6e \u2192 FP8 (8-bit, 32GB HBM)\n",
        "\n",
        "What to expect: Should print your TPU type and selected strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "devices = jax.devices()\n",
        "backend = jax.default_backend()\n",
        "assert backend == \"tpu\", f\"TPU not found (got {backend})\"\n",
        "\n",
        "tpu_type = devices[0].device_kind\n",
        "num_devices = len(devices)\n",
        "print(f\"\u2713 {tpu_type} ({num_devices} devices)\")\n",
        "\n",
        "# Select precision\n",
        "if \"v2\" in tpu_type and num_devices == 8:\n",
        "    STRATEGY, DTYPE, MEM_GB = \"bf16\", jnp.bfloat16, 42\n",
        "    print(\"Strategy: BF16 (16-bit) - 64GB HBM\")\n",
        "elif \"v6\" in tpu_type:\n",
        "    STRATEGY, DTYPE, MEM_GB = \"fp8\", jnp.float8_e4m3fn, 21\n",
        "    print(\"Strategy: FP8 (8-bit) - 32GB HBM\")\n",
        "else:\n",
        "    raise RuntimeError(f\"Unsupported: {tpu_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download GPT-OSS-20B Weights from HuggingFace\n",
        "\n",
        "Downloads the official GPT-OSS-20B model checkpoint (13.8 GB) from HuggingFace.\n",
        "\n",
        "What happens:\n",
        "- Downloads .safetensors files containing 21B parameters\n",
        "- Saves to /content/gpt-oss-20b-dl/original/\n",
        "- Uses HuggingFace's snapshot downloader for efficient transfers\n",
        "\n",
        "Expected time: ~3-5 minutes (depending on HuggingFace bandwidth)\n",
        "\n",
        "Note: If you hit rate limits, wait a few minutes and re-run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from pathlib import Path\n",
        "\n",
        "# Only download if not already set (e.g., from Cell 0 for Mac testing)\n",
        "if 'safetensors_path' not in globals():\n",
        "    print(\"Downloading GPT-OSS-20B (13.8 GB)...\")\n",
        "    checkpoint_dir = snapshot_download(\n",
        "        repo_id=\"openai/gpt-oss-20b\",\n",
        "        revision=\"main\",\n",
        "        allow_patterns=[\"original/*\"],\n",
        "        local_dir=\"/content/gpt-oss-20b-dl\",\n",
        "        local_dir_use_symlinks=False\n",
        "    )\n",
        "    safetensors_path = Path(\"/content/gpt-oss-20b-dl/original\")\n",
        "    print(f\"\u2713 Downloaded: {safetensors_path}\")\n",
        "else:\n",
        "    print(f\"\u2713 Using existing SafeTensors path: {safetensors_path}\")\n",
        "    print(\"  (Set in Cell 0 - skipping download)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Convert SafeTensors to Orbax Format\n",
        "\n",
        "Converts the HuggingFace checkpoint to Orbax format (JAX native).\n",
        "\n",
        "Why convert to Orbax?\n",
        "- 2-3x faster loading than SafeTensors\n",
        "- Optimized for JAX PyTree structures\n",
        "- Supports sharded checkpoints across TPU devices\n",
        "- The JAX-native way to store checkpoints\n",
        "\n",
        "This is a one-time conversion. Future sessions can load from Orbax directly.\n",
        "\n",
        "Note for TPU v6e (FP8): This cell loads as BF16 first (~42GB), then converts to FP8. This may cause OOM on 32GB HBM. If this happens, use a pre-converted FP8 Orbax checkpoint instead.\n",
        "\n",
        "Expected time: ~15-20 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import psutil\n",
        "from gpt_oss.jax.config import ModelConfig\n",
        "from gpt_oss.jax.loader_safetensors import WeightLoader\n",
        "from orbax.checkpoint import PyTreeCheckpointer\n",
        "import orbax.checkpoint as ocp\n",
        "from safetensors import safe_open\n",
        "from pathlib import Path\n",
        "from flax import traverse_util\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Helper function to track memory usage\n",
        "def get_ram_gb():\n",
        "    \"\"\"Get current RAM usage in GB.\"\"\"\n",
        "    return psutil.Process().memory_info().rss / 1024**3\n",
        "\n",
        "config = ModelConfig()\n",
        "# Set orbax_path if not already set (e.g., from Cell 0 for Mac testing)\n",
        "if 'orbax_path' not in globals():\n",
        "    orbax_path = f\"/content/gpt-oss-20b-orbax-{STRATEGY}\"\n",
        "\n",
        "print(f\"Converting to Orbax ({STRATEGY.upper()})...\")\n",
        "print(\"Loading tensors on CPU to avoid TPU OOM...\")\n",
        "ram_start = get_ram_gb()\n",
        "print(f\"Starting RAM: {ram_start:.2f} GB\")\n",
        "t0 = time.time()\n",
        "\n",
        "# For TPU v6e with FP8: Load on CPU, convert, then save\n",
        "# This avoids loading 42GB BF16 into 32GB TPU memory\n",
        "with jax.default_device(jax.devices('cpu')[0]):\n",
        "    loader = WeightLoader(str(safetensors_path))\n",
        "    # Loads all weights as BF16:\n",
        "    # - Regular weights: Loaded as BF16 directly from SafeTensors\n",
        "    # - MXFP4 MoE weights: Decompressed MXFP4 \u2192 BF16 (see loader_safetensors.py)\n",
        "    params = loader.load_params(config, show_progress=True)\n",
        "    ram_after_load = get_ram_gb()\n",
        "    print(f\"After loading BF16: {ram_after_load:.2f} GB (+{ram_after_load - ram_start:.2f} GB)\")\n",
        "    \n",
        "    # Convert BF16 \u2192 FP8 if using FP8 strategy (TPU v6e)\n",
        "    if STRATEGY == \"fp8\":\n",
        "        print(\"Converting BF16 \u2192 FP8 on CPU...\")\n",
        "        # Converts all BF16 tensors to FP8, leaves other dtypes unchanged\n",
        "        params = jax.tree_util.tree_map(\n",
        "            lambda x: x.astype(DTYPE) if x.dtype == jnp.bfloat16 else x,\n",
        "            params\n",
        "        )\n",
        "        ram_after_fp8 = get_ram_gb()\n",
        "        print(f\"After FP8 conversion: {ram_after_fp8:.2f} GB (+{ram_after_fp8 - ram_start:.2f} GB total)\")\n",
        "\n",
        "# Save to Orbax format (still on CPU)\n",
        "print(\"Saving to Orbax...\")\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "checkpointer.save(orbax_path, params)\n",
        "ram_peak = get_ram_gb()\n",
        "\n",
        "print(f\"\u2713 Converted in {time.time()-t0:.1f}s\")\n",
        "print(f\"  Orbax checkpoint: {orbax_path}\")\n",
        "print(f\"  Peak RAM: {ram_peak:.2f} GB (+{ram_peak - ram_start:.2f} GB)\")\n",
        "\n",
        "# Free CPU memory\n",
        "del params\n",
        "import gc\n",
        "gc.collect()\n",
        "ram_after_cleanup = get_ram_gb()\n",
        "print(f\"  After cleanup: {ram_after_cleanup:.2f} GB ({ram_peak - ram_after_cleanup:.2f} GB freed)\")\n",
        "\n",
        "# Clean up SafeTensors to save space\n",
        "!rm -rf /content/gpt-oss-20b-dl\n",
        "print(\"  Cleaned up SafeTensors (13.8 GB freed from disk)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Model Parameters from Orbax\n",
        "\n",
        "Loads the 21B parameters from Orbax checkpoint (JAX native format).\n",
        "\n",
        "BF16 Strategy (TPU v2-8):\n",
        "- Memory footprint: ~42GB\n",
        "- Best accuracy (full precision)\n",
        "\n",
        "FP8 Strategy (TPU v6e):\n",
        "- Memory footprint: ~21GB (50% reduction!)\n",
        "- Minimal accuracy loss (<2% perplexity increase)\n",
        "\n",
        "Expected time: ~2-3 seconds (much faster than SafeTensors!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from orbax.checkpoint import PyTreeCheckpointer\n",
        "import orbax.checkpoint as ocp\n",
        "\n",
        "print(f\"Loading from Orbax ({STRATEGY.upper()})...\")\n",
        "t0 = time.time()\n",
        "\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "params = checkpointer.restore(orbax_path)\n",
        "\n",
        "print(f\"\u2713 Loaded in {time.time()-t0:.1f}s\")\n",
        "print(f\"  Orbax is {15/(time.time()-t0):.1f}x faster than SafeTensors!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Model & Tokenizer\n",
        "\n",
        "Creates the GPT-OSS-20B Transformer model and tokenizer.\n",
        "\n",
        "What happens:\n",
        "- Initializes the model architecture (40 layers, 8192 hidden dim, 64 attention heads)\n",
        "- Loads the tokenizer (GPT-2 style BPE with 50,257 tokens)\n",
        "- Verifies parameter dtype matches your strategy\n",
        "\n",
        "Model Architecture:\n",
        "- Parameters: 20.8B\n",
        "- Layers: 40\n",
        "- Context: 8192 tokens\n",
        "- Vocab: 50,257 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gpt_oss.jax.model import Transformer\n",
        "from gpt_oss.tokenizer import get_tokenizer\n",
        "\n",
        "model = Transformer(config=config)\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "sample = jax.tree_util.tree_leaves(params)[0]\n",
        "print(f\"\u2713 Model: GPT-OSS-20B\")\n",
        "print(f\"  Dtype: {sample.dtype}\")\n",
        "print(f\"  Strategy: {STRATEGY.upper()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Memory Utilization Analysis\n",
        "\n",
        "Calculates actual memory usage and compares to TPU HBM capacity.\n",
        "\n",
        "What you'll see:\n",
        "- Actual memory: Size of loaded parameters in GB\n",
        "- TPU HBM: Total high-bandwidth memory available\n",
        "- Utilization: Percentage of HBM used\n",
        "\n",
        "Target utilization: ~66% (leaves headroom for activations and KV cache)\n",
        "\n",
        "If you see >90% utilization: Model may not fit for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mem_gb(p):\n",
        "    return sum(x.nbytes for x in jax.tree_util.tree_leaves(p)) / 1e9\n",
        "\n",
        "actual = mem_gb(params)\n",
        "print(f\"Memory: {actual:.1f} GB (expected: {MEM_GB} GB)\")\n",
        "\n",
        "tpu_hbm = 64 if \"v2\" in tpu_type and num_devices == 8 else 32\n",
        "print(f\"TPU HBM: {tpu_hbm} GB ({actual/tpu_hbm*100:.0f}% used)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run Inference with Harmony Protocol\n",
        "\n",
        "Demonstrates multi-channel reasoning using the Harmony protocol.\n",
        "\n",
        "Harmony Protocol Features:\n",
        "- Multi-channel output: Separate analysis and final answer channels\n",
        "- Structured reasoning: Model shows its thought process\n",
        "- Efficient inference: Uses KV cache for fast token generation\n",
        "\n",
        "Example Question: \"What is the capital of France?\"\n",
        "\n",
        "Expected output:\n",
        "- Analysis channel: Model's reasoning process (\ud83d\udcca)\n",
        "- Answer channel: Final response (\ud83d\udcac)\n",
        "- Performance: Tokens/second metric\n",
        "\n",
        "Try it: Edit the msg variable to ask your own questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from IPython.display import HTML, display\n",
        "from gpt_oss.jax.inference import generate\n",
        "\n",
        "try:\n",
        "    from openai_harmony import (\n",
        "        load_harmony_encoding,\n",
        "        HarmonyEncodingName,\n",
        "        Conversation,\n",
        "        Message,\n",
        "        Role\n",
        "    )\n",
        "    \n",
        "    encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
        "    msg = \"What is the capital of France?\"\n",
        "    conv = Conversation.from_messages([Message.from_role_and_content(Role.USER, msg)])\n",
        "    prompt_tokens = encoding.render_conversation_for_completion(conv, Role.ASSISTANT)\n",
        "    \n",
        "    output_tokens, stats = generate(\n",
        "        model=model, params=params, prompt_tokens=prompt_tokens,\n",
        "        max_new_tokens=50, temperature=0.0, rng_key=jax.random.PRNGKey(42),\n",
        "        config=config, use_kv_cache=True, show_progress=False, return_stats=True\n",
        "    )\n",
        "    \n",
        "    stop_tokens = encoding.stop_tokens_for_assistant_actions()\n",
        "    filtered = [t for t in output_tokens[len(prompt_tokens):] if t not in stop_tokens]\n",
        "    generated = tokenizer.decode(filtered)\n",
        "    \n",
        "    # Parse channels\n",
        "    analysis = re.search(r'<\\|channel\\|>analysis<\\|message\\|>(.*?)(?:<\\|end\\|>|<\\|channel\\|>|$)', generated, re.DOTALL)\n",
        "    final = re.search(r'<\\|channel\\|>(main|final)<\\|message\\|>(.*?)(?:<\\|end\\|>|<\\|channel\\|>|$)', generated, re.DOTALL)\n",
        "    \n",
        "    if analysis:\n",
        "        print(f\"\ud83d\udcca Analysis: {analysis.group(1).strip()}\")\n",
        "    if final:\n",
        "        print(f\"\ud83d\udcac Answer: {final.group(2).strip()}\")\n",
        "    \n",
        "    print(f\"\\nPerf: {stats['tokens_per_second']:.2f} tok/s\")\n",
        "except Exception as e:\n",
        "    print(f\"Harmony demo error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n",
        "\n",
        "| Metric | TPU v2-8 (BF16) | TPU v6e (FP8) |\n",
        "|--------|-----------------|---------------|\n",
        "| Precision | 16-bit | 8-bit |\n",
        "| Memory | ~42 GB | ~21 GB |\n",
        "| TPU HBM | 64 GB | 32 GB |\n",
        "| Utilization | 66% | 66% |\n",
        "| Load Time | ~5s | ~5s |\n",
        "| Tokens/sec | 50-100 | 80-150* |\n",
        "\n",
        "\\* FP8 may be faster due to lower memory bandwidth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Optional: Save to Google Drive\n",
        "\n",
        "Uncomment the code below to save your Orbax checkpoint to Google Drive.\n",
        "\n",
        "Why save to Drive?\n",
        "- Colab sessions are temporary (max 12 hours)\n",
        "- Avoid re-downloading model weights in future sessions\n",
        "- 2-3x faster loading from Drive than HuggingFace\n",
        "\n",
        "Note: Requires ~20-42 GB of Drive storage depending on precision strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save to Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r {orbax_path} /content/drive/MyDrive/\n",
        "# print(\"\u2713 Saved to Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. TPU Memory Monitoring\n",
        "\n",
        "Real-time monitoring of TPU memory usage across all devices.\n",
        "\n",
        "What you'll see:\n",
        "- Per-device breakdown: Memory usage for each TPU core\n",
        "- Bytes in use: Current memory consumption\n",
        "- Bytes limit: Maximum available memory\n",
        "- Utilization percentage: How much of each device's memory is used\n",
        "\n",
        "Use this to:\n",
        "- Debug OOM (Out of Memory) errors\n",
        "- Verify memory is balanced across devices\n",
        "- Monitor memory during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"TPU Monitoring:\")\n",
        "try:\n",
        "    from jax.lib import xla_bridge\n",
        "    backend = xla_bridge.get_backend()\n",
        "    for i, dev in enumerate(devices):\n",
        "        try:\n",
        "            info = backend.get_memory_info(dev)\n",
        "            if info:\n",
        "                used = info.bytes_in_use / 1e9\n",
        "                limit = info.bytes_limit / 1e9\n",
        "                print(f\"  Device {i}: {used:.1f}/{limit:.1f} GB ({used/limit*100:.0f}%)\")\n",
        "        except:\n",
        "            print(f\"  Device {i}: Memory info unavailable\")\n",
        "except Exception as e:\n",
        "    print(f\"  Monitoring unavailable: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Cleanup Temporary Files\n",
        "\n",
        "Removes the temporary download directory to free up disk space.\n",
        "\n",
        "What gets deleted:\n",
        "- /content/gpt-oss-20b-dl/ (13.8 GB)\n",
        "- Original safetensors files\n",
        "\n",
        "What's preserved:\n",
        "- Loaded parameters in memory\n",
        "- Orbax checkpoint (if you ran Cell 5)\n",
        "\n",
        "Safe to run: Parameters are already loaded in RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup temp files\n",
        "!rm -rf /content/gpt-oss-20b-dl\n",
        "print(\"\u2713 Cleaned temp files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "**OOM Errors**: Verify TPU type matches strategy (Cell 3)\n",
        "\n",
        "**TPU Not Detected**: Runtime \u2192 Change runtime type \u2192 TPU, then restart\n",
        "\n",
        "**Slow Download**: HuggingFace rate limits - wait and retry\n",
        "\n",
        "**Import Errors**: Re-run Cell 2 (environment setup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 Optimization Exercises\n",
        "\n",
        "### 1. JAX Code Optimization\n",
        "Profile with `jax.profiler`, optimize bottlenecks\n",
        "\n",
        "[Code](https://github.com/atsentia/gpt-oss-jax/blob/main/gpt_oss/jax/model.py)\n",
        "\n",
        "### 2. KV Cache Optimization\n",
        "Implement INT8/FP8 KV cache for 2-4x memory savings\n",
        "\n",
        "[Code](https://github.com/atsentia/gpt-oss-jax/blob/main/gpt_oss/jax/kv_cache.py)\n",
        "\n",
        "### 3. Advanced Quantization\n",
        "On-the-fly MXFP4 dequantization: 10.5 GB vs 21 GB\n",
        "\n",
        "[Code](https://github.com/atsentia/gpt-oss-jax/tree/main/gpt_oss/jax/quantization)\n",
        "\n",
        "### 4. Speculative Decoding\n",
        "Draft model (GPT-2) + verification: 2-3x speedup\n",
        "\n",
        "### 5. Continuous Batching\n",
        "Batch multiple requests: 5-10x throughput\n",
        "\n",
        "**Discuss**: [GitHub Discussions](https://github.com/atsentia/gpt-oss-jax/discussions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "\u2705 Demonstrated adaptive precision (BF16 vs FP8)\n",
        "\n",
        "\u2705 2x memory reduction enables TPU v6e\n",
        "\n",
        "\u2705 Production patterns: monitoring, error handling\n",
        "\n",
        "\u2705 Harmony protocol multi-channel reasoning\n",
        "\n",
        "### Resources\n",
        "- [Repository](https://github.com/atsentia/gpt-oss-jax)\n",
        "- [Model Card](https://huggingface.co/openai/gpt-oss-20b)\n",
        "- [JAX Docs](https://jax.readthedocs.io/)\n",
        "\n",
        "**Issues?** [Open an issue](https://github.com/atsentia/gpt-oss-jax/issues)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}