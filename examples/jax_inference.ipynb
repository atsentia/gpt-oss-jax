{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff3cd; border: 2px solid #ffc107; border-radius: 5px; padding: 15px; margin: 10px 0; font-family: Arial, sans-serif;\">\n",
    "  <h2 style=\"color: #856404; margin-top: 0;\">\n",
    "    ‚ö†Ô∏è Performance Notice: CPU-Only Implementation\n",
    "  </h2>\n",
    "  <p style=\"color: #856404; font-size: 14px; line-height: 1.6; margin-bottom: 0;\">\n",
    "    <strong>This notebook runs on CPU only</strong> and is <strong>not yet optimized</strong> for production use.\n",
    "  </p>\n",
    "  <ul style=\"color: #856404; font-size: 14px; line-height: 1.6; margin-top: 10px;\">\n",
    "    <li><strong>No GPU acceleration</strong> - All computation runs on CPU</li>\n",
    "    <li><strong>No Flash Attention</strong> - Using standard attention implementation</li>\n",
    "    <li><strong>No speculative decoding</strong> - Standard autoregressive generation</li>\n",
    "    <li><strong>No quantization optimizations</strong> - Full precision inference</li>\n",
    "  </ul>\n",
    "  <p style=\"color: #856404; font-size: 14px; line-height: 1.6; margin-top: 10px; margin-bottom: 0;\">\n",
    "    Expect <strong>slower generation speeds</strong> compared to optimized GPU implementations. \n",
    "    This is a reference implementation for development and testing purposes.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Device Detection](#device-detection) - Check available compute devices\n",
    "2. [Checkpoint Loading](#checkpoint-loading) - Load model weights (Orbax or SafeTensors)\n",
    "3. [Model Initialization](#model-initialization) - Initialize transformer model and tokenizer\n",
    "4. [Simple Generation](#simple-generation) - Basic text generation example\n",
    "5. [Streaming Generation with Progress Bar](#streaming-generation-with-progress-bar) - Generation with progress tracking\n",
    "6. [Temperature Sampling](#temperature-sampling) - Experiment with different temperatures\n",
    "7. [OpenAI Harmony Prompt Format Example](#openai-harmony-prompt-format-example) - Verify Harmony tokenizer formatting\n",
    "8. [Conclusion](#conclusion) - Summary and next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Inference with GPT-OSS-20B\n",
    "\n",
    "This notebook demonstrates how to run inference with the GPT-OSS-20B model using JAX.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Fast checkpoint loading**: Supports both Orbax (5-6s) and SafeTensors formats\n",
    "- **Efficient generation**: KV caching for autoregressive generation\n",
    "- **Progress tracking**: Real-time progress bars and performance statistics\n",
    "- **Temperature sampling**: Configurable randomness for diverse outputs\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Install dependencies using one of the methods below:\n",
    "\n",
    "**Using uv (recommended):**\n",
    "```bash\n",
    "uv venv\n",
    "uv pip install -e \".[jax,notebook]\"\n",
    "```\n",
    "\n",
    "**Using pip:**\n",
    "```bash\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -e \".[jax,notebook]\"\n",
    "```\n",
    "\n",
    "**Setup Jupyter kernel:**\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "python -m ipykernel install --user --name=jax-for-gpt-oss --display-name \"Python (jax-for-gpt-oss)\"\n",
    "```\n",
    "\n",
    "For detailed installation instructions, see [INSTALL.md](../../INSTALL.md).\n",
    "\n",
    "**Note**: Update the `CHECKPOINT_PATH` variable below to point to your checkpoint directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# GPT-OSS JAX imports\n",
    "from gpt_oss.jax.config import ModelConfig\n",
    "from gpt_oss.jax.model import Transformer\n",
    "from gpt_oss.jax.inference import generate, sample_token\n",
    "from gpt_oss.jax.loader_orbax import OrbaxWeightLoader, load_config_from_orbax\n",
    "from gpt_oss.jax.loader_safetensors import WeightLoader\n",
    "from gpt_oss.tokenizer import get_tokenizer\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# XLA compiler flags for CPU optimization\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_cpu_enable_fast_math=true --xla_cpu_enable_fast_min_max=true\"\n",
    "\n",
    "# Compilation cache directory (speeds up subsequent runs)\n",
    "cache_dir = Path.home() / \".cache\" / \"jax\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"JAX_COMPILATION_CACHE_DIR\"] = str(cache_dir)\n",
    "\n",
    "# Optional: Enable compilation logging (useful for debugging)\n",
    "# os.environ[\"JAX_LOG_COMPILES\"] = \"1\"\n",
    "\n",
    "print(f\"‚úì XLA flags configured\")\n",
    "print(f\"‚úì Compilation cache: ~/.cache/jax\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Compilation cache: ~/.cache/jax\n",
      "‚úì Compilation cache: ~/.cache/jax\n",
      "‚úì Compilation cache: ~/.cache/jax\n",
      "‚úì XLA flags configured\n",
      "‚úì Compilation cache: ~/.cache/jax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# XLA compiler flags for CPU optimization\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_cpu_enable_fast_math=true --xla_cpu_enable_fast_min_max=true\"\n",
    "\n",
    "# Compilation cache directory (speeds up subsequent runs)\n",
    "print(f\"‚úì Compilation cache: ~/.cache/jax\")\n",
    "print(f\"‚úì Compilation cache: ~/.cache/jax\")\n",
    "print(f\"‚úì Compilation cache: ~/.cache/jax\")\n",
    "\n",
    "# Optional: Enable compilation logging (useful for debugging)\n",
    "# os.environ[\"JAX_LOG_COMPILES\"] = \"1\"\n",
    "\n",
    "print(f\"‚úì XLA flags configured\")\n",
    "print(f\"‚úì Compilation cache: ~/.cache/jax\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Detection\n",
    "\n",
    "Check which devices JAX is using for computation. On CPU-only systems, this will show CPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend: cpu\n",
      "Devices: 1\n",
      "  Device 0: cpu (ID: 0)\n"
     ]
    }
   ],
   "source": [
    "# Get available devices\n",
    "devices = jax.devices()\n",
    "backend = jax.default_backend()\n",
    "\n",
    "print(f\"Backend: {backend}\")\n",
    "print(f\"Devices: {len(devices)}\")\n",
    "for i, device in enumerate(devices):\n",
    "    print(f\"  Device {i}: {device.device_kind} (ID: {device.id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint Loading\n",
    "\n",
    "The notebook automatically detects whether you're using an Orbax or SafeTensors checkpoint:\n",
    "\n",
    "- **Orbax**: Pre-converted format, loads in ~5-6 seconds\n",
    "- **SafeTensors**: Original format, includes MXFP4 decompression, loads in ~90 seconds\n",
    "\n",
    "### Setup Checkpoint Path\n",
    "\n",
    "**Recommended**: Create a symlink in the `weights/` directory:\n",
    "\n",
    "```bash\n",
    "# From the repository root\n",
    "ln -s /path/to/your/checkpoint weights/gpt-oss-20b\n",
    "```\n",
    "\n",
    "Then use the relative path in the notebook: `../weights/gpt-oss-20b`\n",
    "\n",
    "**Alternative**: Update `CHECKPOINT_PATH` in the cell below to point directly to your checkpoint directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Detected checkpoint format: orbax\n",
      "‚úì Checkpoint path: ../weights/gpt-oss-20b\n",
      "\n",
      "Loading checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100% [00:05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Loaded 27 top-level parameter groups\n",
      "  ‚úì Total parameters: 20,914,757,184 (20.91B)\n",
      "\n",
      "‚úì Checkpoint loaded in 6.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppress Orbax warnings about sharding info (prevents local path exposure)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Sharding info not provided.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"orbax.*\")\n",
    "# Suppress asyncio errors from Jupyter/IPython kernel\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"asyncio.*\")\n",
    "import logging\n",
    "logging.getLogger(\"asyncio\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Update this path to your checkpoint directory\n",
    "# Recommended: Create a symlink in weights/ directory and use relative path\n",
    "CHECKPOINT_PATH = \"../weights/gpt-oss-20b\"\n",
    "# Example: ln -s /path/to/your/checkpoint weights/gpt-oss-20b\n",
    "\n",
    "# Alternative: Use absolute path directly\n",
    "# CHECKPOINT_PATH = \"/absolute/path/to/checkpoint\"\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = Path(CHECKPOINT_PATH).expanduser().resolve()\n",
    "\n",
    "# Verify path exists\n",
    "if not checkpoint_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Checkpoint path does not exist: {checkpoint_path}\\n\"\n",
    "        f\"Please update CHECKPOINT_PATH to point to your checkpoint directory.\"\n",
    "    )\n",
    "\n",
    "def detect_checkpoint_format(checkpoint_path: Path) -> str:\n",
    "    \"\"\"Detect checkpoint format (Orbax or SafeTensors).\"\"\"\n",
    "    # Check if path exists\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(f\"Checkpoint path does not exist: {checkpoint_path}\")\n",
    "    \n",
    "    # Check for Orbax checkpoint: should have a \"0\" subdirectory with state\n",
    "    if (checkpoint_path / \"0\").exists():\n",
    "        state_path = checkpoint_path / \"0\" / \"state\"\n",
    "        if state_path.exists() and (state_path / \"_METADATA\").exists():\n",
    "            return \"orbax\"\n",
    "        # Alternative: check if state directory exists directly\n",
    "        elif state_path.exists():\n",
    "            # Check for any metadata files that indicate Orbax\n",
    "            if any(state_path.glob(\"*METADATA*\")) or any(state_path.glob(\"*.pkl\")):\n",
    "                return \"orbax\"\n",
    "    \n",
    "    # Check for SafeTensors: should have .safetensors files\n",
    "    safetensor_files = list(checkpoint_path.glob(\"*.safetensors\"))\n",
    "    if safetensor_files:\n",
    "        return \"safetensors\"\n",
    "    \n",
    "    # Provide helpful error message with diagnostics\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not detect checkpoint format. Diagnostics:\")\n",
    "    print(f\"   Path: {CHECKPOINT_PATH}\")\n",
    "    print(f\"   Path exists: {checkpoint_path.exists()}\")\n",
    "    \n",
    "    if checkpoint_path.is_dir():\n",
    "        print(f\"   Contents:\")\n",
    "        try:\n",
    "            contents = list(checkpoint_path.iterdir())[:10]  # First 10 items\n",
    "            for item in contents:\n",
    "                print(f\"     - {item.name} ({'dir' if item.is_dir() else 'file'})\")\n",
    "        except PermissionError:\n",
    "            print(f\"     (Permission denied)\")\n",
    "    \n",
    "    # Check for common issues\n",
    "    if (checkpoint_path / \"0\").exists():\n",
    "        print(f\"   Found '0' subdirectory, checking structure...\")\n",
    "        zero_dir = checkpoint_path / \"0\"\n",
    "        if (zero_dir / \"state\").exists():\n",
    "            state_contents = list((zero_dir / \"state\").iterdir())[:5]\n",
    "            print(f\"   State directory contents: {[c.name for c in state_contents]}\")\n",
    "    \n",
    "    raise ValueError(\n",
    "        f\"Could not detect checkpoint format in {checkpoint_path}\\n\"\n",
    "        f\"Expected either:\\n\"\n",
    "        f\"  - Orbax: checkpoint/0/state/_METADATA exists\\n\"\n",
    "        f\"  - SafeTensors: checkpoint/*.safetensors files exist\\n\"\n",
    "        f\"Please verify your checkpoint path is correct.\"\n",
    "    )\n",
    "\n",
    "# Detect format\n",
    "checkpoint_format = detect_checkpoint_format(checkpoint_path)\n",
    "print(f\"‚úì Detected checkpoint format: {checkpoint_format}\")\n",
    "print(f\"‚úì Checkpoint path: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"\\nLoading checkpoint...\")\n",
    "load_start = time.time()\n",
    "\n",
    "if checkpoint_format == \"orbax\":\n",
    "    loader = OrbaxWeightLoader(str(checkpoint_path))\n",
    "    params = loader.load_params(show_progress=True, unpack_quantized=True)\n",
    "    # Load config from Orbax (hardcoded for GPT-OSS-20B)\n",
    "    config_dict = load_config_from_orbax(str(checkpoint_path))\n",
    "\n",
    "    config = ModelConfig(\n",
    "        num_hidden_layers=config_dict[\"num_hidden_layers\"],\n",
    "        hidden_size=config_dict[\"hidden_size\"],\n",
    "        head_dim=config_dict[\"head_dim\"],\n",
    "        num_attention_heads=config_dict[\"num_attention_heads\"],\n",
    "        num_key_value_heads=config_dict[\"num_key_value_heads\"],\n",
    "        sliding_window=config_dict[\"sliding_window\"],\n",
    "        intermediate_size=config_dict[\"intermediate_size\"],\n",
    "        num_experts=config_dict[\"num_experts\"],\n",
    "        experts_per_token=config_dict[\"experts_per_token\"],\n",
    "        vocab_size=config_dict[\"vocab_size\"],\n",
    "        swiglu_limit=config_dict[\"swiglu_limit\"],\n",
    "        rope_theta=config_dict[\"rope_theta\"],\n",
    "        rope_scaling_factor=config_dict[\"rope_scaling_factor\"],\n",
    "        rope_ntk_alpha=config_dict[\"rope_ntk_alpha\"],\n",
    "        rope_ntk_beta=config_dict[\"rope_ntk_beta\"],\n",
    "        initial_context_length=config_dict[\"initial_context_length\"],\n",
    "    )\n",
    "else:\n",
    "    # SafeTensors: load config first, then weights\n",
    "    config_path = checkpoint_path / \"config.json\"\n",
    "    if config_path.exists():\n",
    "        import json\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        config = ModelConfig(\n",
    "            num_hidden_layers=config_dict.get(\"num_hidden_layers\", 36),\n",
    "            hidden_size=config_dict.get(\"hidden_size\", 2880),\n",
    "            head_dim=config_dict.get(\"head_dim\", 64),\n",
    "            num_attention_heads=config_dict.get(\"num_attention_heads\", 64),\n",
    "            num_key_value_heads=config_dict.get(\"num_key_value_heads\", 8),\n",
    "            sliding_window=config_dict.get(\"sliding_window\", 128),\n",
    "            intermediate_size=config_dict.get(\"intermediate_size\", 2880),\n",
    "            num_experts=config_dict.get(\"num_experts\", 128),\n",
    "            experts_per_token=config_dict.get(\"experts_per_token\", 4),\n",
    "            vocab_size=config_dict.get(\"vocab_size\", 201088),\n",
    "            swiglu_limit=config_dict.get(\"swiglu_limit\", 7.0),\n",
    "            rope_theta=config_dict.get(\"rope_theta\", 150000.0),\n",
    "            rope_scaling_factor=config_dict.get(\"rope_scaling_factor\", 32.0),\n",
    "            rope_ntk_alpha=config_dict.get(\"rope_ntk_alpha\", 1.0),\n",
    "            rope_ntk_beta=config_dict.get(\"rope_ntk_beta\", 32.0),\n",
    "            initial_context_length=config_dict.get(\"initial_context_length\", 4096),\n",
    "        )\n",
    "    else:\n",
    "        # Fallback to defaults\n",
    "        config = ModelConfig()\n",
    "    \n",
    "    loader = WeightLoader(str(checkpoint_path))\n",
    "    params = loader.load_params(config, show_progress=True)\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"\\n‚úì Checkpoint loaded in {load_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Initialize the Transformer model with the GPT-OSS-20B configuration:\n",
    "\n",
    "- **24 transformer layers** (for Orbax checkpoints)\n",
    "- **2880 hidden dimensions**\n",
    "- **64 attention heads** (8 key/value heads with GQA)\n",
    "- **32 MoE experts** (4 experts per token)\n",
    "- **201,088 vocabulary size**\n",
    "\n",
    "Also initialize the tokenizer for encoding/decoding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model initialized\n",
      "  Layers: 24\n",
      "  Hidden size: 2880\n",
      "  Attention heads: 64 (Q), 8 (K/V)\n",
      "  MoE experts: 32 (activating 4 per token)\n",
      "  Vocabulary size: 201088\n",
      "‚úì Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# Create model with config\n",
    "model = Transformer(config=config)\n",
    "\n",
    "# Get tokenizer (uses openai-harmony if available, otherwise manual construction)\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "print(f\"‚úì Model initialized\")\n",
    "print(f\"  Layers: {config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {config.hidden_size}\")\n",
    "print(f\"  Attention heads: {config.num_attention_heads} (Q), {config.num_key_value_heads} (K/V)\")\n",
    "print(f\"  MoE experts: {config.num_experts} (activating {config.experts_per_token} per token)\")\n",
    "print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"‚úì Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Generation\n",
    "\n",
    "Generate text using greedy decoding (temperature=0.0). The first token will include JIT compilation time, which is cached for subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who wrote Romeo and Juliet?\n",
      "Prompt tokens (6): [20600, 11955, 96615, 326, 128971, 30]...\n",
      "\n",
      "Generating tokens...\n",
      "[KV Cache] Initialized 24 caches, shape: (1, 4096, 8, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                     | 1/10 [00:06<00:55,  6.16s/tok, last_token=410, ttft=6.15s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Token 0] Detailed timing, cache_offset=6:\n",
      "  Input shape: (6,)\n",
      "  Array creation: 3.52ms\n",
      "  Forward pass: 6.14s\n",
      "  Total token time: 6.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                              | 2/10 [00:08<00:30,  3.82s/tok, last_token=4066, tok_s=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Token 1] Detailed timing, cache_offset=7:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 3.31ms\n",
      "  Forward pass: 2.17s\n",
      "  Total token time: 2.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 3/10 [00:09<00:18,  2.61s/tok, last_token=256, tok_s=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Token 2] Detailed timing, cache_offset=8:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.12ms\n",
      "  Forward pass: 1.16s\n",
      "  Total token time: 1.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.80s/tok, last_token=17, tok_s=0.56]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Output:\n",
      "Who wrote Romeo and Juliet?**  \n",
      "   *Answer: William Shakespeare*\n",
      "\n",
      "2\n",
      "\n",
      "============================================================\n",
      "Statistics:\n",
      "  Generated tokens: 10\n",
      "  Total time: 17.95s\n",
      "  First token time (TTFT): 6.15s\n",
      "  Tokens/second: 0.56\n",
      "  Tokens/second (after first): 0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up prompt\n",
    "prompt = \"Who wrote Romeo and Juliet?\"\n",
    "prompt_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Prompt tokens ({len(prompt_tokens)}): {prompt_tokens[:10]}...\")\n",
    "\n",
    "# Create RNG key (not needed for greedy, but included for consistency)\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Generate tokens\n",
    "print(f\"\\nGenerating tokens...\")\n",
    "gen_start = time.time()\n",
    "\n",
    "output_tokens, stats = generate(\n",
    "    model=model,\n",
    "    params=params,\n",
    "    prompt_tokens=prompt_tokens,\n",
    "    max_new_tokens=10,  # Short output\n",
    "    temperature=0.0,  # Greedy decoding\n",
    "    rng_key=rng_key,\n",
    "    config=config,\n",
    "    use_kv_cache=True,\n",
    "    show_progress=True,\n",
    "    return_stats=True\n",
    ")\n",
    "\n",
    "gen_time = time.time() - gen_start\n",
    "\n",
    "# Decode output\n",
    "output_text = tokenizer.decode(output_tokens)\n",
    "generated_tokens = output_tokens[len(prompt_tokens):]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Output:\")\n",
    "print(f\"{output_text}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Generated tokens: {len(generated_tokens)}\")\n",
    "print(f\"  Total time: {stats['total_time']:.2f}s\")\n",
    "print(f\"  First token time (TTFT): {stats['first_token_time']:.2f}s\")\n",
    "print(f\"  Tokens/second: {stats['tokens_per_second']:.2f}\")\n",
    "if stats['tokens_per_second_after_first'] > 0:\n",
    "    print(f\"  Tokens/second (after first): {stats['tokens_per_second_after_first']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Generation with Progress Bar\n",
    "\n",
    "Generate tokens with a progress bar and detailed statistics. The `generate()` function supports `show_progress=True` and `return_stats=True` for real-time feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of artificial intelligence\n",
      "\n",
      "Generating with temperature=0.8...\n",
      "[KV Cache] Initialized 24 caches, shape: (1, 4096, 8, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   2%|‚ñà‚ñè                                                         | 1/50 [00:05<04:26,  5.43s/tok, last_token=350, ttft=5.26s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Token 0] Detailed timing, cache_offset=5:\n",
      "  Input shape: (5,)\n",
      "  Array creation: 5.71ms\n",
      "  Forward pass: 5.25s\n",
      "  Total token time: 5.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   4%|‚ñà‚ñà‚ñé                                                      | 2/50 [00:06<02:17,  2.87s/tok, last_token=17527, tok_s=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Token 1] Detailed timing, cache_offset=6:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.29ms\n",
      "  Forward pass: 1.07s\n",
      "  Total token time: 1.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   6%|‚ñà‚ñà‚ñà‚ñã                                                         | 3/50 [00:07<01:30,  1.93s/tok, last_token=8, tok_s=0.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Token 2] Detailed timing, cache_offset=7:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.11ms\n",
      "  Forward pass: 0.81s\n",
      "  Total token time: 0.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:06<00:00,  1.32s/tok, last_token=410, tok_s=0.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Generated text:\n",
      "The future of artificial intelligence (AI) is a topic that has garnered significant attention in both the scientific community and popular media.\n",
      "\n",
      "In this article, we will explore the current state of AI research and development, as well as the potential applications and challenges that lie ahead.\n",
      "\n",
      "**\n",
      "\n",
      "============================================================\n",
      "Performance Statistics:\n",
      "  First token time (TTFT): 5.440s\n",
      "  Total generation time: 66.14s\n",
      "  Tokens/second: 0.76\n",
      "  Tokens/second (after first): 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_with_progress(model, params, prompt_tokens, max_new_tokens=100, \n",
    "                           temperature=0.8, config=None):\n",
    "    \"\"\"Generate tokens with progress bar using existing generate() function.\"\"\"\n",
    "    rng_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # Track timing\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    \n",
    "    # Use token_callback to track first token\n",
    "    def token_callback(token):\n",
    "        nonlocal first_token_time\n",
    "        if first_token_time is None:\n",
    "            first_token_time = time.time() - start_time\n",
    "    \n",
    "    # Generate with progress bar\n",
    "    output_tokens, stats = generate(\n",
    "        model=model,\n",
    "        params=params,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        rng_key=rng_key,\n",
    "        config=config,\n",
    "        use_kv_cache=True,\n",
    "        show_progress=True,\n",
    "        token_callback=token_callback,\n",
    "        return_stats=True\n",
    "    )\n",
    "    \n",
    "    # Extract stats\n",
    "    result_stats = {\n",
    "        \"first_token_time\": first_token_time or stats.get('first_token_time', 0),\n",
    "        \"total_time\": stats['total_time'],\n",
    "        \"tokens_per_second\": stats['tokens_per_second'],\n",
    "        \"tokens_per_second_after_first\": stats.get('tokens_per_second_after_first', 0),\n",
    "    }\n",
    "    \n",
    "    return output_tokens, result_stats\n",
    "\n",
    "# Test with a new prompt\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "prompt_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGenerating with temperature={0.8}...\")\n",
    "\n",
    "output_tokens, stats = generate_with_progress(\n",
    "    model=model,\n",
    "    params=params,\n",
    "    prompt_tokens=prompt_tokens,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.8,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Decode and display\n",
    "output_text = tokenizer.decode(output_tokens)\n",
    "generated_tokens = output_tokens[len(prompt_tokens):]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generated text:\")\n",
    "print(f\"{output_text}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Performance Statistics:\")\n",
    "print(f\"  First token time (TTFT): {stats['first_token_time']:.3f}s\")\n",
    "print(f\"  Total generation time: {stats['total_time']:.2f}s\")\n",
    "print(f\"  Tokens/second: {stats['tokens_per_second']:.2f}\")\n",
    "if stats['tokens_per_second_after_first'] > 0:\n",
    "    print(f\"  Tokens/second (after first): {stats['tokens_per_second_after_first']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Sampling\n",
    "\n",
    "Experiment with different temperature values to control generation randomness:\n",
    "\n",
    "- **Temperature = 0.0**: Greedy decoding (deterministic, always picks most likely token)\n",
    "- **Temperature = 0.5**: Low randomness (mostly deterministic with slight variation)\n",
    "- **Temperature = 1.0**: Balanced randomness (default for most use cases)\n",
    "- **Temperature = 1.5**: High randomness (more creative but potentially less coherent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Once upon a time'\n",
      "\n",
      "Generating with different temperatures...\n",
      "============================================================\n",
      "\n",
      "Temperature = 0.0:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Token 0] Detailed timing, cache_offset=4:\n",
      "  Input shape: (4,)\n",
      "  Array creation: 5.99ms\n",
      "  Forward pass: 4.59s\n",
      "  Total token time: 4.59s\n",
      "\n",
      "[Token 1] Detailed timing, cache_offset=5:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.05ms\n",
      "  Forward pass: 1.15s\n",
      "  Total token time: 1.15s\n",
      "\n",
      "[Token 2] Detailed timing, cache_offset=6:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.04ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n",
      "Generated: , in a small town, there lived a young boy named Alex. Alex was a curious and adventurous child, always eager to explore the world around him\n",
      "Time: 29.48s, Speed: 1.02 tok/s\n",
      "\n",
      "Temperature = 0.5:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Token 0] Detailed timing, cache_offset=4:\n",
      "  Input shape: (4,)\n",
      "  Array creation: 0.05ms\n",
      "  Forward pass: 2.89s\n",
      "  Total token time: 2.89s\n",
      "\n",
      "[Token 1] Detailed timing, cache_offset=5:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.06ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n",
      "\n",
      "[Token 2] Detailed timing, cache_offset=6:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.05ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n",
      "Generated: , in a small town named Willowbrook, there lived a young girl named Lily. Lily had a curious mind and a heart full of kindness. She\n",
      "Time: 27.65s, Speed: 1.09 tok/s\n",
      "\n",
      "Temperature = 1.0:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Token 0] Detailed timing, cache_offset=4:\n",
      "  Input shape: (4,)\n",
      "  Array creation: 0.04ms\n",
      "  Forward pass: 2.88s\n",
      "  Total token time: 2.88s\n",
      "\n",
      "[Token 1] Detailed timing, cache_offset=5:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.06ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n",
      "\n",
      "[Token 2] Detailed timing, cache_offset=6:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.04ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n",
      "Generated: , a group of friends decided to hold a competition to see who could come up with the most creative way to say \"I love you.\" They each\n",
      "Time: 27.72s, Speed: 1.08 tok/s\n",
      "\n",
      "Temperature = 1.5:\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Token 0] Detailed timing, cache_offset=4:\n",
      "  Input shape: (4,)\n",
      "  Array creation: 0.05ms\n",
      "  Forward pass: 2.90s\n",
      "  Total token time: 2.90s\n",
      "\n",
      "[Token 1] Detailed timing, cache_offset=5:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.06ms\n",
      "  Forward pass: 0.86s\n",
      "  Total token time: 0.86s\n",
      "\n",
      "[Token 2] Detailed timing, cache_offset=6:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.04ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n",
      "Generated: , a developer discovered a code that could automatically send an email and convert each instance into a PDF. To unlock the secret, he decided to go with\n",
      "Time: 27.78s, Speed: 1.08 tok/s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "prompt_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"\\nGenerating with different temperatures...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    output_tokens, stats = generate(\n",
    "        model=model,\n",
    "        params=params,\n",
    "        prompt_tokens=prompt_tokens,\n",
    "        max_new_tokens=30,\n",
    "        temperature=temp,\n",
    "        rng_key=jax.random.PRNGKey(42 + int(temp * 100)),  # Different seed per temp\n",
    "        config=config,\n",
    "        use_kv_cache=True,\n",
    "        show_progress=False,  # Disable progress bar for cleaner output\n",
    "        return_stats=True\n",
    "    )\n",
    "    \n",
    "    output_text = tokenizer.decode(output_tokens)\n",
    "    generated_text = output_text[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(f\"Time: {stats['total_time']:.2f}s, Speed: {stats['tokens_per_second']:.2f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Harmony Prompt Format Example\n",
    "\n",
    "A simple example demonstrating proper Harmony tokenizer formatting with special tokens (`<|startoftext|>`, `<|message|>`, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Harmony Format Example with Color-Coded Output\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 15px 0; padding: 15px; background-color: #e3f2fd; border-left: 4px solid #2196f3; border-radius: 4px;\">\n",
       "    <h3 style=\"margin-top: 0; color: #1976d2; font-family: Arial, sans-serif;\">\n",
       "        a) Original User Message\n",
       "    </h3>\n",
       "    <p style=\"font-size: 16px; font-family: monospace; color: #0d47a1; margin: 0;\">\n",
       "        What is the capital of France?\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 15px 0; padding: 15px; background-color: #e8f5e9; border-left: 4px solid #4caf50; border-radius: 4px;\">\n",
       "    <h3 style=\"margin-top: 0; color: #2e7d32; font-family: Arial, sans-serif;\">\n",
       "        b) Harmony-Formatted Prompt (with special tokens)\n",
       "    </h3>\n",
       "    <p style=\"font-size: 14px; font-family: monospace; color: #1b5e20; margin: 0; white-space: pre-wrap; word-break: break-all;\">\n",
       "        &lt;|start|&gt;user&lt;|message|&gt;What is the capital of France?&lt;|end|&gt;&lt;|start|&gt;assistant\n",
       "    </p>\n",
       "    <p style=\"font-size: 12px; color: #558b2f; margin-top: 10px; font-family: Arial, sans-serif;\">\n",
       "        ‚úì Prompt tokens: 13 | Special tokens: [200006, 200008, 200007, 200006]\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "[Token 0] Detailed timing, cache_offset=13:\n",
      "  Input shape: (13,)\n",
      "  Array creation: 6.64ms\n",
      "  Forward pass: 14.84s\n",
      "  Total token time: 14.85s\n",
      "\n",
      "[Token 1] Detailed timing, cache_offset=14:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.05ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.86s\n",
      "\n",
      "[Token 2] Detailed timing, cache_offset=15:\n",
      "  Input shape: (1,)\n",
      "  Array creation: 0.04ms\n",
      "  Forward pass: 0.85s\n",
      "  Total token time: 0.85s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 15px 0; padding: 15px; background-color: #fff9c4; border-left: 4px solid #fbc02d; border-radius: 4px;\">\n",
       "    <h3 style=\"margin-top: 0; color: #f57f17; font-family: Arial, sans-serif;\">\n",
       "        c) Full Generated Response (raw Harmony format)\n",
       "    </h3>\n",
       "    <p style=\"font-size: 14px; font-family: monospace; color: #827717; margin: 0; white-space: pre-wrap; word-break: break-all;\">\n",
       "        &lt;|channel|&gt;analysis&lt;|message|&gt;We need to answer the question: \"What is the capital of France?\" The answer: Paris.&lt;|end|&gt;&lt;|start|&gt;assistant&lt;|channel|&gt;final&lt;|message|&gt;The capital of France is **Paris**.\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 15px 0;\">\n",
       "    <h3 style=\"color: #424242; font-family: Arial, sans-serif; margin-bottom: 10px;\">\n",
       "        d) Parsed Harmony Output (extracted channels)\n",
       "    </h3>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"margin: 15px 0; padding: 15px; background-color: #f3e5f5; border-left: 4px solid #ab47bc; border-radius: 4px;\">\n",
       "    <h4 style=\"margin-top: 0; color: #7b1fa2; font-family: Arial, sans-serif;\">\n",
       "        üí¨ Final Answer (final channel)\n",
       "    </h4>\n",
       "    <div style=\"font-size: 15px; color: #4a148c; font-family: Arial, sans-serif;\">\n",
       "        <p style='margin: 5px 0;'>[TextContent(text='We need to answer the question: \"What is the capital of France?\" The answer: Paris.')]</p><br><p style='margin: 5px 0;'>[TextContent(text='The capital of France is **Paris**.')]</p>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance: 61.38s | 0.81 tok/s\n"
     ]
    }
   ],
   "source": [
    "# Harmony-formatted prompt example with colored output\n",
    "# Import necessary modules\n",
    "import jax\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "from gpt_oss.jax.inference import generate\n",
    "from gpt_oss.tokenizer import get_tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# User message\n",
    "user_message = \"What is the capital of France?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Harmony Format Example with Color-Coded Output\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display a) Original user message\n",
    "display(HTML(f\"\"\"\n",
    "<div style=\"margin: 15px 0; padding: 15px; background-color: #e3f2fd; border-left: 4px solid #2196f3; border-radius: 4px;\">\n",
    "    <h3 style=\"margin-top: 0; color: #1976d2; font-family: Arial, sans-serif;\">\n",
    "        a) Original User Message\n",
    "    </h3>\n",
    "    <p style=\"font-size: 16px; font-family: monospace; color: #0d47a1; margin: 0;\">\n",
    "        {user_message}\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "# Try to use openai-harmony for proper Harmony formatting\n",
    "try:\n",
    "    from openai_harmony import (\n",
    "        load_harmony_encoding,\n",
    "        HarmonyEncodingName,\n",
    "        Conversation,\n",
    "        Message,\n",
    "        Role\n",
    "    )\n",
    "    \n",
    "    # Load Harmony encoding\n",
    "    encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n",
    "    \n",
    "    # Create Harmony conversation\n",
    "    conversation = Conversation.from_messages([\n",
    "        Message.from_role_and_content(Role.USER, user_message)\n",
    "    ])\n",
    "    \n",
    "    # Render conversation for completion (adds Harmony special tokens)\n",
    "    prompt_tokens = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n",
    "    \n",
    "    # Decode prompt to show Harmony formatting\n",
    "    prompt_text = tokenizer.decode(prompt_tokens)\n",
    "    \n",
    "    # Display b) Harmony-formatted prompt\n",
    "    # Escape HTML special characters but preserve Harmony tokens\n",
    "    prompt_text_escaped = prompt_text.replace('<', '&lt;').replace('>', '&gt;')\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "<div style=\"margin: 15px 0; padding: 15px; background-color: #e8f5e9; border-left: 4px solid #4caf50; border-radius: 4px;\">\n",
    "    <h3 style=\"margin-top: 0; color: #2e7d32; font-family: Arial, sans-serif;\">\n",
    "        b) Harmony-Formatted Prompt (with special tokens)\n",
    "    </h3>\n",
    "    <p style=\"font-size: 14px; font-family: monospace; color: #1b5e20; margin: 0; white-space: pre-wrap; word-break: break-all;\">\n",
    "        {prompt_text_escaped}\n",
    "    </p>\n",
    "    <p style=\"font-size: 12px; color: #558b2f; margin-top: 10px; font-family: Arial, sans-serif;\">\n",
    "        ‚úì Prompt tokens: {len(prompt_tokens)} | Special tokens: {[t for t in prompt_tokens if t >= 199998]}\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "    \n",
    "    # Get Harmony stop tokens\n",
    "    stop_token_ids = encoding.stop_tokens_for_assistant_actions()\n",
    "    \n",
    "    # Ensure model, params, config are available\n",
    "    if \"model\" in globals() and \"params\" in globals() and \"config\" in globals():\n",
    "        print(\"\\nGenerating response...\")\n",
    "        \n",
    "        output_tokens, stats = generate(\n",
    "            model=model,\n",
    "            params=params,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.0,  # Greedy decoding\n",
    "            rng_key=jax.random.PRNGKey(42),\n",
    "            config=config,\n",
    "            use_kv_cache=True,\n",
    "            show_progress=False,\n",
    "            return_stats=True\n",
    "        )\n",
    "        \n",
    "        # Filter out stop tokens\n",
    "        filtered_tokens = []\n",
    "        for token in output_tokens[len(prompt_tokens):]:\n",
    "            if token in stop_token_ids:\n",
    "                break\n",
    "            filtered_tokens.append(token)\n",
    "        \n",
    "        # Extract just the generated part (without the prompt)\n",
    "        generated_only = tokenizer.decode(filtered_tokens)\n",
    "        generated_only_escaped = generated_only.replace('<', '&lt;').replace('>', '&gt;')\n",
    "        \n",
    "        # Display c) Full generated response\n",
    "        display(HTML(f\"\"\"\n",
    "<div style=\"margin: 15px 0; padding: 15px; background-color: #fff9c4; border-left: 4px solid #fbc02d; border-radius: 4px;\">\n",
    "    <h3 style=\"margin-top: 0; color: #f57f17; font-family: Arial, sans-serif;\">\n",
    "        c) Full Generated Response (raw Harmony format)\n",
    "    </h3>\n",
    "    <p style=\"font-size: 14px; font-family: monospace; color: #827717; margin: 0; white-space: pre-wrap; word-break: break-all;\">\n",
    "        {generated_only_escaped}\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "        \n",
    "        # Parse Harmony output to separate reasoning from final answer\n",
    "        try:\n",
    "            # Parse completion tokens into structured messages\n",
    "            completion_tokens = filtered_tokens\n",
    "            messages = encoding.parse_messages_from_completion_tokens(completion_tokens, Role.ASSISTANT)\n",
    "            \n",
    "            # Extract reasoning and final answer from parsed messages\n",
    "            reasoning_parts = []\n",
    "            answer_parts = []\n",
    "            \n",
    "            for msg in messages:\n",
    "                # Check message structure - Harmony messages have recipient/channel info\n",
    "                msg_dict = msg.to_dict() if hasattr(msg, 'to_dict') else {}\n",
    "                \n",
    "                # Look for channel information\n",
    "                recipient = getattr(msg, 'recipient', None) or msg_dict.get('recipient', '')\n",
    "                content = getattr(msg, 'content', None) or msg_dict.get('content', '')\n",
    "                \n",
    "                if 'analysis' in str(recipient).lower() or 'analysis' in str(content).lower():\n",
    "                    reasoning_parts.append(str(content))\n",
    "                elif 'main' in str(recipient).lower() or 'final' in str(recipient).lower() or (not recipient and content):\n",
    "                    answer_parts.append(str(content))\n",
    "                else:\n",
    "                    # Try to extract from content string\n",
    "                    content_str = str(content)\n",
    "                    if '<|channel|>analysis' in content_str:\n",
    "                        reasoning_parts.append(content_str)\n",
    "                    elif '<|channel|>main' in content_str or '<|channel|>final' in content_str:\n",
    "                        answer_parts.append(content_str)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If parsing fails, try manual parsing\n",
    "            print(f\"Harmony parsing failed ({e}), using manual parsing...\")\n",
    "            \n",
    "            # Extract reasoning (analysis channel)\n",
    "            reasoning_match = re.search(r'<\\|channel\\|>analysis<\\|message\\|>(.*?)(?:<\\|end\\|>|<\\|channel\\|>|$)', generated_only, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning_parts = [reasoning_match.group(1).strip()]\n",
    "            \n",
    "            # Extract final answer (main/final channel)\n",
    "            answer_match = re.search(r'<\\|channel\\|>(main|final)<\\|message\\|>(.*?)(?:<\\|end\\|>|<\\|channel\\|>|$)', generated_only, re.DOTALL)\n",
    "            if answer_match:\n",
    "                answer_parts = [answer_match.group(2).strip()]\n",
    "        \n",
    "        # Display d) Parsed output\n",
    "        display(HTML(f\"\"\"\n",
    "<div style=\"margin: 15px 0;\">\n",
    "    <h3 style=\"color: #424242; font-family: Arial, sans-serif; margin-bottom: 10px;\">\n",
    "        d) Parsed Harmony Output (extracted channels)\n",
    "    </h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "        \n",
    "        # Display reasoning (green)\n",
    "        if reasoning_parts:\n",
    "            reasoning_html = \"<br>\".join([f\"<p style='margin: 5px 0;'>{str(part)}</p>\" for part in reasoning_parts])\n",
    "            display(HTML(f\"\"\"\n",
    "<div style=\"margin: 15px 0; padding: 15px; background-color: #c8e6c9; border-left: 4px solid #66bb6a; border-radius: 4px;\">\n",
    "    <h4 style=\"margin-top: 0; color: #2e7d32; font-family: Arial, sans-serif;\">\n",
    "        üìä Reasoning/Analysis (analysis channel)\n",
    "    </h4>\n",
    "    <div style=\"font-size: 15px; color: #1b5e20; font-family: Arial, sans-serif;\">\n",
    "        {reasoning_html}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "        \n",
    "        # Display final answer (magenta/purple)\n",
    "        if answer_parts:\n",
    "            answer_html = \"<br>\".join([f\"<p style='margin: 5px 0;'>{str(part)}</p>\" for part in answer_parts])\n",
    "            display(HTML(f\"\"\"\n",
    "<div style=\"margin: 15px 0; padding: 15px; background-color: #f3e5f5; border-left: 4px solid #ab47bc; border-radius: 4px;\">\n",
    "    <h4 style=\"margin-top: 0; color: #7b1fa2; font-family: Arial, sans-serif;\">\n",
    "        üí¨ Final Answer (final channel)\n",
    "    </h4>\n",
    "    <div style=\"font-size: 15px; color: #4a148c; font-family: Arial, sans-serif;\">\n",
    "        {answer_html}\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"))\n",
    "        \n",
    "        # Display stats\n",
    "        print(f\"\\nPerformance: {stats['total_time']:.2f}s | {stats['tokens_per_second']:.2f} tok/s\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Please run the Model Initialization and Checkpoint Loading sections first.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  openai-harmony not available\")\n",
    "    print(\"   Install with: pip install openai-harmony\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Checkpoint loading**: Fast loading from both Orbax and SafeTensors formats\n",
    "2. **Model initialization**: Setting up the GPT-OSS-20B transformer model\n",
    "3. **Text generation**: Greedy and temperature-based sampling\n",
    "4. **Performance tracking**: Measuring TTFT (time to first token) and tokens/second\n",
    "5. **Compilation caching**: Leveraging JAX's compilation cache for faster subsequent runs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Milestone 3**: Chat integration with streaming responses\n",
    "- **Milestone 4**: PyPI package preparation\n",
    "- **Milestone 5**: Advanced optimizations (FlashAttention, quantization)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [JAX Documentation](https://jax.readthedocs.io/)\n",
    "- [Flax Documentation](https://flax.readthedocs.io/)\n",
    "- [GPT-OSS-20B Model Card](https://huggingface.co/atsentia/gpt-oss-20b)\n",
    "\n",
    "Happy generating! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jax-for-gpt-oss)",
   "language": "python",
   "name": "jax-for-gpt-oss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
