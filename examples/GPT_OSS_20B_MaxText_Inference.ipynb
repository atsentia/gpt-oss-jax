{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# GPT-OSS 20B Inference with MaxText on Google Colab TPU\n",
    "\n",
    "This notebook demonstrates how to run inference with OpenAI's GPT-OSS 20B model using Google's MaxText framework on TPU v6e or v5e.\n",
    "\n",
    "## Model Details\n",
    "- **Model**: GPT-OSS 20B (21B parameters)\n",
    "- **Architecture**: MoE (32 experts, 4 active per token)\n",
    "- **Context Length**: 128K tokens (YaRN scaled RoPE)\n",
    "- **Quantization**: MXFP4 (mixed precision) → BF16 → Optional INT8\n",
    "\n",
    "## Workflow\n",
    "1. Download MXFP4 checkpoint from HuggingFace\n",
    "2. Dequantize MXFP4 → BF16 (GPU required)\n",
    "3. Convert BF16 → MaxText unscanned format (for inference)\n",
    "4. **Optional**: Quantize to INT8 for 10-12% faster inference on TPU v6e\n",
    "5. Run inference with MaxText\n",
    "\n",
    "## Requirements\n",
    "- **Runtime**: TPU v2-8 (minimum), TPU v6e-8 (recommended for int8)\n",
    "- **Storage**: ~100GB GCS bucket\n",
    "- **GPU**: T4 or better for MXFP4 dequantization (Step 2)\n",
    "- **HuggingFace Token**: Required for model download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_runtime"
   },
   "outputs": [],
   "source": [
    "# Check if running on TPU\n",
    "import os\n",
    "import jax\n",
    "\n",
    "# Check available devices\n",
    "devices = jax.devices()\n",
    "print(f\"Available devices: {devices}\")\n",
    "print(f\"Device type: {devices[0].platform}\")\n",
    "\n",
    "if devices[0].platform != 'tpu':\n",
    "    print(\"⚠️  WARNING: Not running on TPU! Please change runtime to TPU v2-8 or higher.\")\n",
    "    print(\"   Go to Runtime → Change runtime type → Hardware accelerator → TPU\")\n",
    "else:\n",
    "    print(f\"✅ Running on {len(devices)} TPU cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install MaxText and dependencies\n",
    "!git clone https://github.com/AI-Hypercomputer/maxtext.git /content/maxtext\n",
    "%cd /content/maxtext\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -U jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install -r requirements.txt\n",
    "!pip install huggingface-hub safetensors tqdm\n",
    "\n",
    "print(\"✅ MaxText installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_vars"
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ============ USER CONFIGURATION ============\n",
    "# Replace with your HuggingFace token\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # Get from https://huggingface.co/settings/tokens\n",
    "\n",
    "# Replace with your GCS bucket (must have write access)\n",
    "GCS_BUCKET = \"gs://your-bucket-name\"  # e.g., \"gs://my-maxtext-models\"\n",
    "\n",
    "# Enable INT8 quantization for faster inference on TPU v6e (recommended)\n",
    "USE_INT8_QUANTIZATION = True\n",
    "# ============================================\n",
    "\n",
    "# Verify configuration\n",
    "if HF_TOKEN == \"YOUR_HUGGINGFACE_TOKEN_HERE\":\n",
    "    raise ValueError(\"Please set your HuggingFace token in the cell above\")\n",
    "\n",
    "if GCS_BUCKET == \"gs://your-bucket-name\":\n",
    "    raise ValueError(\"Please set your GCS bucket path in the cell above\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "BASE_OUTPUT_PATH = f\"{GCS_BUCKET}/gpt-oss-20b/{timestamp}\"\n",
    "\n",
    "# Local paths\n",
    "LOCAL_MXFP4_PATH = \"/content/gpt-oss-20b-mxfp4\"\n",
    "LOCAL_BF16_PATH = \"/content/gpt-oss-20b-bf16\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"gpt-oss-20b\"\n",
    "TOKENIZER_PATH = \"openai/gpt-oss-20b\"\n",
    "\n",
    "print(f\"✅ Configuration complete\")\n",
    "print(f\"   Output path: {BASE_OUTPUT_PATH}\")\n",
    "print(f\"   INT8 quantization: {USE_INT8_QUANTIZATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## Step 1: Download GPT-OSS 20B (MXFP4 Format)\n",
    "\n",
    "Download the model from HuggingFace. The model is ~42GB in MXFP4 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_model"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(\"Downloading GPT-OSS 20B (MXFP4 format, ~42GB)...\")\n",
    "print(\"This may take 10-20 minutes depending on network speed.\")\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"openai/gpt-oss-20b\",\n",
    "    local_dir=LOCAL_MXFP4_PATH,\n",
    "    token=HF_TOKEN,\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Model downloaded to {LOCAL_MXFP4_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dequantize"
   },
   "source": [
    "## Step 2: Dequantize MXFP4 → BF16\n",
    "\n",
    "**⚠️ IMPORTANT**: This step requires a GPU (T4 or better). If you're on TPU runtime:\n",
    "1. Temporarily switch to GPU runtime (Runtime → Change runtime type → GPU)\n",
    "2. Run this cell only\n",
    "3. Switch back to TPU runtime\n",
    "\n",
    "Alternatively, run this step locally with CUDA and upload the BF16 checkpoint to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dequantize_mxfp4"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"⚠️  WARNING: No GPU detected!\")\n",
    "    print(\"   This step requires GPU. Options:\")\n",
    "    print(\"   1. Switch to GPU runtime temporarily\")\n",
    "    print(\"   2. Skip this cell if you already have BF16 checkpoint in GCS\")\n",
    "    print(\"   3. Run dequantization locally and upload to GCS\")\n",
    "else:\n",
    "    print(f\"✅ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(\"Starting MXFP4 → BF16 dequantization...\")\n",
    "    print(\"This will take ~15-30 minutes.\")\n",
    "    \n",
    "    # Run dequantization\n",
    "    !python3 -m MaxText.utils.ckpt_scripts.dequantize_mxfp4 \\\n",
    "        --input-path={LOCAL_MXFP4_PATH} \\\n",
    "        --output-path={LOCAL_BF16_PATH} \\\n",
    "        --dtype-str=bf16 \\\n",
    "        --cache-size=2\n",
    "    \n",
    "    print(f\"✅ Dequantization complete: {LOCAL_BF16_PATH}\")\n",
    "    \n",
    "    # Upload BF16 checkpoint to GCS\n",
    "    print(\"Uploading BF16 checkpoint to GCS...\")\n",
    "    !gcloud storage cp -r {LOCAL_BF16_PATH} {BASE_OUTPUT_PATH}/hf-bf16\n",
    "    print(\"✅ BF16 checkpoint uploaded to GCS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "convert"
   },
   "source": [
    "## Step 3: Convert to MaxText Format (Unscanned)\n",
    "\n",
    "Convert the BF16 checkpoint to MaxText's unscanned format, optimized for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convert_unscanned"
   },
   "outputs": [],
   "source": [
    "# If BF16 checkpoint is in GCS, download it first\n",
    "# Skip if you just uploaded it in Step 2\n",
    "import os\n",
    "\n",
    "if not os.path.exists(LOCAL_BF16_PATH):\n",
    "    print(\"Downloading BF16 checkpoint from GCS...\")\n",
    "    !gcloud storage cp -r {BASE_OUTPUT_PATH}/hf-bf16 {LOCAL_BF16_PATH}\n",
    "    print(\"✅ BF16 checkpoint downloaded\")\n",
    "\n",
    "# Convert to unscanned format (inference-optimized)\n",
    "print(\"Converting BF16 → MaxText unscanned format...\")\n",
    "print(\"This may take 10-15 minutes.\")\n",
    "\n",
    "!JAX_PLATFORMS=cpu python3 -m MaxText.utils.ckpt_scripts.convert_gpt_oss_unscanned_ckpt \\\n",
    "    --base-model-path {LOCAL_BF16_PATH} \\\n",
    "    --maxtext-model-path {BASE_OUTPUT_PATH}/unscanned \\\n",
    "    --model-size {MODEL_NAME}\n",
    "\n",
    "UNSCANNED_CKPT_PATH = f\"{BASE_OUTPUT_PATH}/unscanned/0/items\"\n",
    "print(f\"✅ Unscanned checkpoint ready: {UNSCANNED_CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quantize_int8"
   },
   "source": [
    "## Step 4 (Optional): Quantize to INT8 for Faster Inference\n",
    "\n",
    "INT8 quantization provides **10-12% faster inference** on TPU v6e with minimal accuracy loss.\n",
    "\n",
    "**Benefits:**\n",
    "- 10-12% higher FLOPs utilization (measured on Gemma models)\n",
    "- Lower memory bandwidth usage\n",
    "- INT8 input × BF16 weights = BF16 output (efficient on TPU)\n",
    "\n",
    "**Skip this step if:**\n",
    "- You want maximum accuracy (BF16 baseline)\n",
    "- You're not on TPU v6e (v5e also benefits, but less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quantize_to_int8"
   },
   "outputs": [],
   "source": [
    "if USE_INT8_QUANTIZATION:\n",
    "    print(\"Quantizing model to INT8...\")\n",
    "    print(\"This process loads the model layer-by-layer and quantizes on-the-fly.\")\n",
    "    print(\"Estimated time: 20-30 minutes.\")\n",
    "    \n",
    "    # Note: For GPT-OSS, we need to use the general quantization approach\n",
    "    # since layerwise_quantization.py currently only supports DeepSeek models\n",
    "    # We'll use the config-based quantization during inference instead\n",
    "    \n",
    "    print(\"✅ INT8 quantization will be applied during inference (config-based)\")\n",
    "    print(\"   MaxText will use int8 input with bf16 compute automatically.\")\n",
    "    QUANTIZATION_MODE = \"int8\"\n",
    "else:\n",
    "    print(\"Skipping INT8 quantization (using BF16 baseline)\")\n",
    "    QUANTIZATION_MODE = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## Step 5: Run Inference\n",
    "\n",
    "Now we'll run inference with MaxText on TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_config"
   },
   "outputs": [],
   "source": [
    "# Inference configuration\n",
    "PROMPTS = [\n",
    "    \"I love to\",\n",
    "    \"The future of AI is\",\n",
    "    \"Once upon a time\",\n",
    "    \"In a world where\"\n",
    "]\n",
    "\n",
    "# Generation settings\n",
    "MAX_PREFILL_LENGTH = 64  # Max prompt tokens\n",
    "MAX_GENERATION_LENGTH = 128  # Max tokens to generate\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "# TPU configuration (auto-detect)\n",
    "num_tpu_cores = len(jax.devices())\n",
    "if num_tpu_cores == 8:\n",
    "    # v2-8, v3-8, v4-8, v5litepod-8, v5e-8, v6e-8\n",
    "    ICI_FSDP_PARALLELISM = 1\n",
    "    ICI_TENSOR_PARALLELISM = 8\n",
    "elif num_tpu_cores == 16:\n",
    "    ICI_FSDP_PARALLELISM = 1\n",
    "    ICI_TENSOR_PARALLELISM = 16\n",
    "else:\n",
    "    # Default for other configurations\n",
    "    ICI_FSDP_PARALLELISM = 1\n",
    "    ICI_TENSOR_PARALLELISM = num_tpu_cores\n",
    "\n",
    "print(f\"TPU configuration: {num_tpu_cores} cores\")\n",
    "print(f\"Parallelism: FSDP={ICI_FSDP_PARALLELISM}, Tensor={ICI_TENSOR_PARALLELISM}\")\n",
    "print(f\"Quantization: {QUANTIZATION_MODE if QUANTIZATION_MODE else 'BF16 baseline'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_inference"
   },
   "outputs": [],
   "source": [
    "# Run inference for each prompt\n",
    "import subprocess\n",
    "\n",
    "for i, prompt in enumerate(PROMPTS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt {i+1}/{len(PROMPTS)}: '{prompt}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [\n",
    "        \"python3\", \"-m\", \"MaxText.decode\",\n",
    "        \"src/MaxText/configs/base.yml\",\n",
    "        f\"base_output_directory={BASE_OUTPUT_PATH}\",\n",
    "        f\"run_name=inference_prompt_{i+1}\",\n",
    "        f\"model_name={MODEL_NAME}\",\n",
    "        \"tokenizer_type=huggingface\",\n",
    "        f\"tokenizer_path={TOKENIZER_PATH}\",\n",
    "        f\"hf_access_token={HF_TOKEN}\",\n",
    "        f\"load_parameters_path={UNSCANNED_CKPT_PATH}\",\n",
    "        \"scan_layers=False\",\n",
    "        \"attention=dot_product\",\n",
    "        \"sparse_matmul=True\",\n",
    "        \"megablox=True\",\n",
    "        \"dtype=bfloat16\",\n",
    "        \"weight_dtype=bfloat16\",\n",
    "        \"per_device_batch_size=1\",\n",
    "        f\"max_prefill_predict_length={MAX_PREFILL_LENGTH}\",\n",
    "        f\"max_target_length={MAX_GENERATION_LENGTH}\",\n",
    "        f\"prompt={prompt}\",\n",
    "        f\"ici_fsdp_parallelism={ICI_FSDP_PARALLELISM}\",\n",
    "        f\"ici_tensor_parallelism={ICI_TENSOR_PARALLELISM}\",\n",
    "    ]\n",
    "    \n",
    "    # Add INT8 quantization if enabled\n",
    "    if QUANTIZATION_MODE:\n",
    "        cmd.extend([\n",
    "            f\"quantization={QUANTIZATION_MODE}\",\n",
    "            \"quantization_local_shard_count=-1\",  # Auto-detect\n",
    "        ])\n",
    "    \n",
    "    # Run inference\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Parse output (MaxText prints generated text to stdout)\n",
    "    print(\"\\n--- Generated Output ---\")\n",
    "    if result.returncode == 0:\n",
    "        # Extract generated text from output\n",
    "        output_lines = result.stdout.split('\\n')\n",
    "        for line in output_lines:\n",
    "            if 'Generated text:' in line or 'Output:' in line:\n",
    "                print(line)\n",
    "        # Also print last few lines which often contain the output\n",
    "        print('\\n'.join(output_lines[-10:]))\n",
    "    else:\n",
    "        print(\"Error during inference:\")\n",
    "        print(result.stderr)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced_inference"
   },
   "source": [
    "## Step 6: Advanced Inference Configuration\n",
    "\n",
    "For more control over generation, use these advanced settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "advanced_config"
   },
   "outputs": [],
   "source": [
    "# Advanced inference with sampling parameters\n",
    "def run_inference_advanced(\n",
    "    prompt: str,\n",
    "    max_tokens: int = 128,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 40,\n",
    "    enable_kv_quantization: bool = False,\n",
    "    kv_quant_dtype: str = \"int8\"\n",
    "):\n",
    "    \"\"\"Run inference with advanced sampling parameters.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text prompt\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.0 = greedy, 1.0 = random)\n",
    "        top_p: Nucleus sampling threshold\n",
    "        top_k: Top-k sampling threshold\n",
    "        enable_kv_quantization: Quantize KV cache (saves memory)\n",
    "        kv_quant_dtype: KV cache quantization dtype (\"int8\" or \"int4\")\n",
    "    \"\"\"\n",
    "    \n",
    "    cmd = [\n",
    "        \"python3\", \"-m\", \"MaxText.decode\",\n",
    "        \"src/MaxText/configs/base.yml\",\n",
    "        f\"base_output_directory={BASE_OUTPUT_PATH}\",\n",
    "        \"run_name=advanced_inference\",\n",
    "        f\"model_name={MODEL_NAME}\",\n",
    "        \"tokenizer_type=huggingface\",\n",
    "        f\"tokenizer_path={TOKENIZER_PATH}\",\n",
    "        f\"hf_access_token={HF_TOKEN}\",\n",
    "        f\"load_parameters_path={UNSCANNED_CKPT_PATH}\",\n",
    "        \"scan_layers=False\",\n",
    "        \"attention=dot_product\",\n",
    "        \"sparse_matmul=True\",\n",
    "        \"megablox=True\",\n",
    "        \"dtype=bfloat16\",\n",
    "        \"weight_dtype=bfloat16\",\n",
    "        \"per_device_batch_size=1\",\n",
    "        f\"max_prefill_predict_length={MAX_PREFILL_LENGTH}\",\n",
    "        f\"max_target_length={max_tokens}\",\n",
    "        f\"prompt={prompt}\",\n",
    "        f\"ici_fsdp_parallelism={ICI_FSDP_PARALLELISM}\",\n",
    "        f\"ici_tensor_parallelism={ICI_TENSOR_PARALLELISM}\",\n",
    "        f\"temperature={temperature}\",\n",
    "        f\"top_p={top_p}\",\n",
    "        f\"top_k={top_k}\",\n",
    "    ]\n",
    "    \n",
    "    # Add quantization settings\n",
    "    if QUANTIZATION_MODE:\n",
    "        cmd.extend([\n",
    "            f\"quantization={QUANTIZATION_MODE}\",\n",
    "            \"quantization_local_shard_count=-1\",\n",
    "        ])\n",
    "    \n",
    "    # Add KV cache quantization\n",
    "    if enable_kv_quantization:\n",
    "        cmd.extend([\n",
    "            \"quantize_kvcache=True\",\n",
    "            \"kv_quant_axis=heads_and_dkv\",  # Faster, slightly lower accuracy\n",
    "            f\"kv_quant_dtype={kv_quant_dtype}\",\n",
    "        ])\n",
    "    \n",
    "    print(f\"Running inference with:\")\n",
    "    print(f\"  Temperature: {temperature}\")\n",
    "    print(f\"  Top-p: {top_p}\")\n",
    "    print(f\"  Top-k: {top_k}\")\n",
    "    print(f\"  KV quantization: {enable_kv_quantization} ({kv_quant_dtype})\")\n",
    "    print(f\"\\nPrompt: '{prompt}'\\n\")\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        output_lines = result.stdout.split('\\n')\n",
    "        print(\"\\n--- Generated Output ---\")\n",
    "        print('\\n'.join(output_lines[-10:]))\n",
    "    else:\n",
    "        print(\"Error:\")\n",
    "        print(result.stderr)\n",
    "\n",
    "# Example usage\n",
    "run_inference_advanced(\n",
    "    prompt=\"Write a short story about a robot learning to paint:\",\n",
    "    max_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    enable_kv_quantization=True,  # Enable for memory savings\n",
    "    kv_quant_dtype=\"int8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark"
   },
   "source": [
    "## Step 7: Performance Benchmarking\n",
    "\n",
    "Compare BF16 vs INT8 inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_benchmark"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_inference(quantization_mode: str, num_runs: int = 3):\n",
    "    \"\"\"Benchmark inference throughput.\n",
    "    \n",
    "    Args:\n",
    "        quantization_mode: \"\" (BF16) or \"int8\"\n",
    "        num_runs: Number of benchmark runs\n",
    "    \"\"\"\n",
    "    \n",
    "    test_prompt = \"The quick brown fox jumps over the lazy dog\" * 5  # ~50 tokens\n",
    "    \n",
    "    cmd = [\n",
    "        \"python3\", \"-m\", \"MaxText.decode\",\n",
    "        \"src/MaxText/configs/base.yml\",\n",
    "        f\"base_output_directory={BASE_OUTPUT_PATH}\",\n",
    "        f\"run_name=benchmark_{quantization_mode or 'bf16'}\",\n",
    "        f\"model_name={MODEL_NAME}\",\n",
    "        \"tokenizer_type=huggingface\",\n",
    "        f\"tokenizer_path={TOKENIZER_PATH}\",\n",
    "        f\"hf_access_token={HF_TOKEN}\",\n",
    "        f\"load_parameters_path={UNSCANNED_CKPT_PATH}\",\n",
    "        \"scan_layers=False\",\n",
    "        \"attention=dot_product\",\n",
    "        \"sparse_matmul=True\",\n",
    "        \"megablox=True\",\n",
    "        \"dtype=bfloat16\",\n",
    "        \"weight_dtype=bfloat16\",\n",
    "        \"per_device_batch_size=1\",\n",
    "        \"max_prefill_predict_length=64\",\n",
    "        \"max_target_length=128\",\n",
    "        f\"prompt={test_prompt}\",\n",
    "        f\"ici_fsdp_parallelism={ICI_FSDP_PARALLELISM}\",\n",
    "        f\"ici_tensor_parallelism={ICI_TENSOR_PARALLELISM}\",\n",
    "    ]\n",
    "    \n",
    "    if quantization_mode:\n",
    "        cmd.extend([\n",
    "            f\"quantization={quantization_mode}\",\n",
    "            \"quantization_local_shard_count=-1\",\n",
    "        ])\n",
    "    \n",
    "    timings = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        print(f\"Run {i+1}/{num_runs}...\", end=\" \")\n",
    "        start_time = time.time()\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            timings.append(elapsed)\n",
    "            print(f\"{elapsed:.2f}s\")\n",
    "        else:\n",
    "            print(\"Failed\")\n",
    "    \n",
    "    if timings:\n",
    "        avg_time = sum(timings) / len(timings)\n",
    "        tokens_per_sec = 128 / avg_time  # Assuming 128 generated tokens\n",
    "        print(f\"\\nResults for {quantization_mode or 'BF16'}:\")\n",
    "        print(f\"  Average time: {avg_time:.2f}s\")\n",
    "        print(f\"  Tokens/sec: {tokens_per_sec:.2f}\")\n",
    "        return avg_time, tokens_per_sec\n",
    "    else:\n",
    "        print(\"Benchmark failed\")\n",
    "        return None, None\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK: BF16 Baseline\")\n",
    "print(\"=\"*80)\n",
    "bf16_time, bf16_tps = benchmark_inference(\"\", num_runs=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK: INT8 Quantized\")\n",
    "print(\"=\"*80)\n",
    "int8_time, int8_tps = benchmark_inference(\"int8\", num_runs=3)\n",
    "\n",
    "# Compare results\n",
    "if bf16_time and int8_time:\n",
    "    speedup = (bf16_time / int8_time - 1) * 100\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"BF16:  {bf16_tps:.2f} tokens/sec\")\n",
    "    print(f\"INT8:  {int8_tps:.2f} tokens/sec\")\n",
    "    print(f\"Speedup: {speedup:+.1f}%\")\n",
    "    print(\"\\nNote: Expected speedup on TPU v6e is ~10-12%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## Step 8: Cleanup (Optional)\n",
    "\n",
    "Remove local files to free up disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup_files"
   },
   "outputs": [],
   "source": [
    "# Clean up local files (checkpoints are still in GCS)\n",
    "import shutil\n",
    "\n",
    "print(\"Cleaning up local files...\")\n",
    "\n",
    "if os.path.exists(LOCAL_MXFP4_PATH):\n",
    "    shutil.rmtree(LOCAL_MXFP4_PATH)\n",
    "    print(f\"✅ Removed {LOCAL_MXFP4_PATH}\")\n",
    "\n",
    "if os.path.exists(LOCAL_BF16_PATH):\n",
    "    shutil.rmtree(LOCAL_BF16_PATH)\n",
    "    print(f\"✅ Removed {LOCAL_BF16_PATH}\")\n",
    "\n",
    "print(f\"\\nCheckpoints are still available in GCS: {BASE_OUTPUT_PATH}\")\n",
    "print(\"To delete GCS files: !gcloud storage rm -r {BASE_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notes"
   },
   "source": [
    "## Notes and Tips\n",
    "\n",
    "### INT8 Quantization Performance\n",
    "- **TPU v6e**: 10-12% faster inference (int8 input, bf16 compute)\n",
    "- **TPU v5e/v5p**: 6-10% faster inference\n",
    "- **Memory**: ~50% reduction with int8 weights + int8 KV cache\n",
    "\n",
    "### Model Architecture (GPT-OSS 20B)\n",
    "- 24 layers × 32 experts per layer = 768 total experts\n",
    "- 4 experts active per token (sparse MoE)\n",
    "- Grouped Query Attention (64 Q heads, 8 KV heads)\n",
    "- YaRN RoPE for 128K context (scaled from 4K)\n",
    "\n",
    "### Recommended Settings\n",
    "- **Greedy decoding**: `temperature=0.0`\n",
    "- **Creative writing**: `temperature=0.8, top_p=0.95`\n",
    "- **Factual tasks**: `temperature=0.3, top_p=0.9`\n",
    "- **Code generation**: `temperature=0.2, top_k=40`\n",
    "\n",
    "### Troubleshooting\n",
    "1. **Out of memory**: Enable KV cache quantization (`quantize_kvcache=True, kv_quant_dtype=int4`)\n",
    "2. **Slow inference**: Check `sparse_matmul=True` and `megablox=True` are enabled\n",
    "3. **Quality issues**: Reduce temperature or disable INT8 quantization\n",
    "\n",
    "### References\n",
    "- MaxText GitHub: https://github.com/AI-Hypercomputer/maxtext\n",
    "- GPT-OSS Paper: https://openai.com/gpt-oss\n",
    "- Model Card: https://huggingface.co/openai/gpt-oss-20b"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "GPT-OSS 20B MaxText Inference",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
