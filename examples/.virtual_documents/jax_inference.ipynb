


# Standard imports
import jax
import jax.numpy as jnp
from pathlib import Path
import time
from tqdm.auto import tqdm

# GPT-OSS JAX imports
from gpt_oss.jax.config import ModelConfig
from gpt_oss.jax.model import Transformer
from gpt_oss.jax.inference import generate, sample_token
from gpt_oss.jax.loader_orbax import OrbaxWeightLoader, load_config_from_orbax
from gpt_oss.jax.loader_safetensors import WeightLoader
from gpt_oss.tokenizer import get_tokenizer

print("✓ All imports successful")





import os

# XLA compiler flags for CPU optimization
os.environ["XLA_FLAGS"] = "--xla_cpu_enable_fast_math=true --xla_cpu_enable_fast_min_max=true"

# Compilation cache directory (speeds up subsequent runs)
cache_dir = Path.home() / ".cache" / "jax"
cache_dir.mkdir(parents=True, exist_ok=True)
os.environ["JAX_COMPILATION_CACHE_DIR"] = str(cache_dir)

# Optional: Enable compilation logging (useful for debugging)
# os.environ["JAX_LOG_COMPILES"] = "1"

print(f"✓ XLA flags configured")
print(f"✓ Compilation cache: {cache_dir}")





# Get available devices
devices = jax.devices()
backend = jax.default_backend()

print(f"Backend: {backend}")
print(f"Devices: {len(devices)}")
for i, device in enumerate(devices):
    print(f"  Device {i}: {device.device_kind} (ID: {device.id})")





# Update this path to your checkpoint directory
CHECKPOINT_PATH = "../atsentia-gpt-oss-experiments/gpt-oss-jax-orbax/orbax_checkpoints/gpt-oss-20b"
# Alternative: Use absolute path
# CHECKPOINT_PATH = "/absolute/path/to/checkpoint"

checkpoint_path = Path(CHECKPOINT_PATH).expanduser().resolve()

def detect_checkpoint_format(checkpoint_path: Path) -> str:
    """Detect checkpoint format (Orbax or SafeTensors)."""
    # Check for Orbax: should have "0" subdirectory with state
    if (checkpoint_path / "0").exists():
        state_path = checkpoint_path / "0" / "state"
        if state_path.exists() and (state_path / "_METADATA").exists():
            return "orbax"
    
    # Check for SafeTensors: should have .safetensors files
    if list(checkpoint_path.glob("*.safetensors")):
        return "safetensors"
    
    raise ValueError(f"Could not detect checkpoint format in {checkpoint_path}")

# Detect format
checkpoint_format = detect_checkpoint_format(checkpoint_path)
print(f"✓ Detected checkpoint format: {checkpoint_format}")
print(f"✓ Checkpoint path: {checkpoint_path}")

# Load checkpoint
print(f"\nLoading checkpoint...")
load_start = time.time()

if checkpoint_format == "orbax":
    loader = OrbaxWeightLoader(str(checkpoint_path))
    params = loader.load_params(show_progress=True, unpack_quantized=True)
    # Load config from Orbax (hardcoded for GPT-OSS-20B)
    config_dict = load_config_from_orbax(str(checkpoint_path))
    config = ModelConfig(
        num_hidden_layers=config_dict["num_hidden_layers"],
        hidden_size=config_dict["hidden_size"],
        head_dim=config_dict["head_dim"],
        num_attention_heads=config_dict["num_attention_heads"],
        num_key_value_heads=config_dict["num_key_value_heads"],
        sliding_window=config_dict["sliding_window"],
        intermediate_size=config_dict["intermediate_size"],
        num_experts=config_dict["num_experts"],
        experts_per_token=config_dict["experts_per_token"],
        vocab_size=config_dict["vocab_size"],
        swiglu_limit=config_dict["swiglu_limit"],
        rope_theta=config_dict["rope_theta"],
        rope_scaling_factor=config_dict["rope_scaling_factor"],
        rope_ntk_alpha=config_dict["rope_ntk_alpha"],
        rope_ntk_beta=config_dict["rope_ntk_beta"],
        initial_context_length=config_dict["initial_context_length"],
    )
else:
    # SafeTensors: load config first, then weights
    config_path = checkpoint_path / "config.json"
    if config_path.exists():
        import json
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        config = ModelConfig(
            num_hidden_layers=config_dict.get("num_hidden_layers", 36),
            hidden_size=config_dict.get("hidden_size", 2880),
            head_dim=config_dict.get("head_dim", 64),
            num_attention_heads=config_dict.get("num_attention_heads", 64),
            num_key_value_heads=config_dict.get("num_key_value_heads", 8),
            sliding_window=config_dict.get("sliding_window", 128),
            intermediate_size=config_dict.get("intermediate_size", 2880),
            num_experts=config_dict.get("num_experts", 128),
            experts_per_token=config_dict.get("experts_per_token", 4),
            vocab_size=config_dict.get("vocab_size", 201088),
            swiglu_limit=config_dict.get("swiglu_limit", 7.0),
            rope_theta=config_dict.get("rope_theta", 150000.0),
            rope_scaling_factor=config_dict.get("rope_scaling_factor", 32.0),
            rope_ntk_alpha=config_dict.get("rope_ntk_alpha", 1.0),
            rope_ntk_beta=config_dict.get("rope_ntk_beta", 32.0),
            initial_context_length=config_dict.get("initial_context_length", 4096),
        )
    else:
        # Fallback to defaults
        config = ModelConfig()
    
    loader = WeightLoader(str(checkpoint_path))
    params = loader.load_params(config, show_progress=True)

load_time = time.time() - load_start
print(f"\n✓ Checkpoint loaded in {load_time:.2f}s")





# Create model with config
model = Transformer(config=config)

# Get tokenizer
tokenizer = get_tokenizer()

print(f"✓ Model initialized")
print(f"  Layers: {config.num_hidden_layers}")
print(f"  Hidden size: {config.hidden_size}")
print(f"  Attention heads: {config.num_attention_heads} (Q), {config.num_key_value_heads} (K/V)")
print(f"  MoE experts: {config.num_experts} (activating {config.experts_per_token} per token)")
print(f"  Vocabulary size: {config.vocab_size}")
print(f"✓ Tokenizer loaded")





# Set up prompt
prompt = "Who wrote Romeo and Juliet?"
prompt_tokens = tokenizer.encode(prompt)

print(f"Prompt: {prompt}")
print(f"Prompt tokens ({len(prompt_tokens)}): {prompt_tokens[:10]}...")

# Create RNG key (not needed for greedy, but included for consistency)
rng_key = jax.random.PRNGKey(42)

# Generate tokens
print(f"\nGenerating tokens...")
gen_start = time.time()

output_tokens, stats = generate(
    model=model,
    params=params,
    prompt_tokens=prompt_tokens,
    max_new_tokens=20,
    temperature=0.0,  # Greedy decoding
    rng_key=rng_key,
    config=config,
    use_kv_cache=True,
    show_progress=True,
    return_stats=True
)

gen_time = time.time() - gen_start

# Decode output
output_text = tokenizer.decode(output_tokens)
generated_tokens = output_tokens[len(prompt_tokens):]

print(f"\n{'='*60}")
print(f"Output:")
print(f"{output_text}")
print(f"\n{'='*60}")
print(f"Statistics:")
print(f"  Generated tokens: {len(generated_tokens)}")
print(f"  Total time: {stats['total_time']:.2f}s")
print(f"  First token time (TTFT): {stats['first_token_time']:.2f}s")
print(f"  Tokens/second: {stats['tokens_per_second']:.2f}")
if stats['tokens_per_second_after_first'] > 0:
    print(f"  Tokens/second (after first): {stats['tokens_per_second_after_first']:.2f}")





def generate_with_progress(model, params, prompt_tokens, max_new_tokens=100, 
                           temperature=0.8, config=None):
    """Generate tokens with progress bar using existing generate() function."""
    rng_key = jax.random.PRNGKey(42)
    
    # Track timing
    start_time = time.time()
    first_token_time = None
    
    # Use token_callback to track first token
    def token_callback(token):
        nonlocal first_token_time
        if first_token_time is None:
            first_token_time = time.time() - start_time
    
    # Generate with progress bar
    output_tokens, stats = generate(
        model=model,
        params=params,
        prompt_tokens=prompt_tokens,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        rng_key=rng_key,
        config=config,
        use_kv_cache=True,
        show_progress=True,
        token_callback=token_callback,
        return_stats=True
    )
    
    # Extract stats
    result_stats = {
        "first_token_time": first_token_time or stats.get('first_token_time', 0),
        "total_time": stats['total_time'],
        "tokens_per_second": stats['tokens_per_second'],
        "tokens_per_second_after_first": stats.get('tokens_per_second_after_first', 0),
    }
    
    return output_tokens, result_stats

# Test with a new prompt
prompt = "The future of artificial intelligence"
prompt_tokens = tokenizer.encode(prompt)

print(f"Prompt: {prompt}")
print(f"\nGenerating with temperature={0.8}...")

output_tokens, stats = generate_with_progress(
    model=model,
    params=params,
    prompt_tokens=prompt_tokens,
    max_new_tokens=50,
    temperature=0.8,
    config=config
)

# Decode and display
output_text = tokenizer.decode(output_tokens)
generated_tokens = output_tokens[len(prompt_tokens):]

print(f"\n{'='*60}")
print(f"Generated text:")
print(f"{output_text}")
print(f"\n{'='*60}")
print(f"Performance Statistics:")
print(f"  First token time (TTFT): {stats['first_token_time']:.3f}s")
print(f"  Total generation time: {stats['total_time']:.2f}s")
print(f"  Tokens/second: {stats['tokens_per_second']:.2f}")
if stats['tokens_per_second_after_first'] > 0:
    print(f"  Tokens/second (after first): {stats['tokens_per_second_after_first']:.2f}")





prompt = "Explain quantum computing in simple terms."
prompt_tokens = tokenizer.encode(prompt)

# Cold start (first run - includes JIT compilation)
print("Cold start (first run - includes JIT compilation)...")
cold_start_time = time.time()
output_tokens_cold, stats_cold = generate(
    model=model,
    params=params,
    prompt_tokens=prompt_tokens,
    max_new_tokens=30,
    temperature=0.0,
    rng_key=jax.random.PRNGKey(42),
    config=config,
    use_kv_cache=True,
    show_progress=True,
    return_stats=True
)
cold_time = time.time() - cold_start_time

print(f"\n{'='*60}")
print("Warm start (second run - uses compilation cache)...")
warm_start_time = time.time()
output_tokens_warm, stats_warm = generate(
    model=model,
    params=params,
    prompt_tokens=prompt_tokens,
    max_new_tokens=30,
    temperature=0.0,
    rng_key=jax.random.PRNGKey(42),
    config=config,
    use_kv_cache=True,
    show_progress=True,
    return_stats=True
)
warm_time = time.time() - warm_start_time

print(f"\n{'='*60}")
print("Performance Comparison:")
print(f"  Cold start:")
print(f"    Total time: {stats_cold['total_time']:.2f}s")
print(f"    First token: {stats_cold['first_token_time']:.2f}s")
print(f"    Tokens/second: {stats_cold['tokens_per_second']:.2f}")
print(f"  Warm start:")
print(f"    Total time: {stats_warm['total_time']:.2f}s")
print(f"    First token: {stats_warm['first_token_time']:.2f}s")
print(f"    Tokens/second: {stats_warm['tokens_per_second']:.2f}")

speedup = stats_cold['total_time'] / stats_warm['total_time']
print(f"\n  Speedup from compilation cache: {speedup:.2f}x")





prompt = "Once upon a time"
prompt_tokens = tokenizer.encode(prompt)

temperatures = [0.0, 0.5, 1.0, 1.5]

print(f"Prompt: '{prompt}'")
print(f"\nGenerating with different temperatures...")
print(f"{'='*60}")

for temp in temperatures:
    print(f"\nTemperature = {temp}:")
    print("-" * 60)
    
    output_tokens, stats = generate(
        model=model,
        params=params,
        prompt_tokens=prompt_tokens,
        max_new_tokens=30,
        temperature=temp,
        rng_key=jax.random.PRNGKey(42 + int(temp * 100)),  # Different seed per temp
        config=config,
        use_kv_cache=True,
        show_progress=False,  # Disable progress bar for cleaner output
        return_stats=True
    )
    
    output_text = tokenizer.decode(output_tokens)
    generated_text = output_text[len(prompt):].strip()
    
    print(f"Generated: {generated_text}")
    print(f"Time: {stats['total_time']:.2f}s, Speed: {stats['tokens_per_second']:.2f} tok/s")



